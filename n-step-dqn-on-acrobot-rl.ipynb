{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4559fa91",
   "metadata": {
    "papermill": {
     "duration": 0.005784,
     "end_time": "2023-07-01T20:41:59.391702",
     "exception": false,
     "start_time": "2023-07-01T20:41:59.385918",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Deep Q-Learning and Variants in Gym's Acrobot\n",
    "In this notebook, we will explore the implementation of a Deep Q-Learning (DQN) agent to navigate Gym's Acrobot environment.\n",
    "\n",
    "We will use apply two variants of the DQN algorithm:\n",
    "\n",
    "- The Classic DQN (Mihn et al 2013)\n",
    "- N-Step DQN (2, 3 and 4 steps)\n",
    "- More to be added in future\n",
    "\n",
    "The Acrobot environment tasks an agent with controlling a two-link robot to reach a specific height starting from a hanging position. The agent can apply torque to the joint between the two links, aiming to swing the acrobot, build momentum, and achieve the target height in as few steps as possible. The challenge is managing the system's physics, where limited direct control requires the agent to strategically apply torque at the right times to reach the goal.\n",
    "\n",
    "All reinforcement learning (RL) methods will be built from scratch, providing a comprehensive understanding of their workings and we will use PyTorch to build our neural network model.\n",
    "\n",
    "Let's initialize an Acrobot-v1 environmnet, make random actions in the environment, then view a recording of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2264f43f",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-07-01T20:41:59.405022Z",
     "iopub.status.busy": "2023-07-01T20:41:59.404113Z",
     "iopub.status.idle": "2023-07-01T20:43:17.750163Z",
     "shell.execute_reply": "2023-07-01T20:43:17.748673Z"
    },
    "papermill": {
     "duration": 78.356099,
     "end_time": "2023-07-01T20:43:17.753196",
     "exception": false,
     "start_time": "2023-07-01T20:41:59.397097",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting swig\r\n",
      "  Downloading swig-4.1.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: swig\r\n",
      "Successfully installed swig-4.1.1\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0mRequirement already satisfied: gym[box2d] in /opt/conda/lib/python3.10/site-packages (0.26.2)\r\n",
      "Requirement already satisfied: numpy>=1.18.0 in /opt/conda/lib/python3.10/site-packages (from gym[box2d]) (1.23.5)\r\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gym[box2d]) (2.2.1)\r\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /opt/conda/lib/python3.10/site-packages (from gym[box2d]) (0.0.8)\r\n",
      "Collecting box2d-py==2.3.5 (from gym[box2d])\r\n",
      "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25hCollecting pygame==2.1.0 (from gym[box2d])\r\n",
      "  Downloading pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: swig==4.* in /opt/conda/lib/python3.10/site-packages (from gym[box2d]) (4.1.1)\r\n",
      "Building wheels for collected packages: box2d-py\r\n",
      "  Building wheel for box2d-py (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=494650 sha256=df1370c2d3a0dcc376ba51f96d84b849cb8fa04ecbccd4c9525276295ad3f3f4\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\r\n",
      "Successfully built box2d-py\r\n",
      "Installing collected packages: box2d-py, pygame\r\n",
      "Successfully installed box2d-py-2.3.5 pygame-2.1.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install swig \n",
    "!pip install gym[box2d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4a1d817",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-01T20:43:17.782293Z",
     "iopub.status.busy": "2023-07-01T20:43:17.781880Z",
     "iopub.status.idle": "2023-07-01T20:43:18.232069Z",
     "shell.execute_reply": "2023-07-01T20:43:18.230700Z"
    },
    "papermill": {
     "duration": 0.469198,
     "end_time": "2023-07-01T20:43:18.234993",
     "exception": false,
     "start_time": "2023-07-01T20:43:17.765795",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('Acrobot-v1')\n",
    "env.reset(seed=42)\n",
    "\n",
    "# Play one complete episode with random actions\n",
    "while True:\n",
    "    action = env.action_space.sample() \n",
    "    _, _, terminated, truncated, _ = env.step(action)\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d001a80",
   "metadata": {
    "papermill": {
     "duration": 0.012882,
     "end_time": "2023-07-01T20:43:18.261148",
     "exception": false,
     "start_time": "2023-07-01T20:43:18.248266",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "![](https://i.imgur.com/0bdwRIt.gif)\n",
    "\n",
    "## Acrobot General Information\n",
    "This information is from the official Gym documentation.\n",
    "\n",
    "https://www.gymlibrary.dev/environments/classic/acrobot/\n",
    "\n",
    "| Feature Category  | Details                                |\n",
    "|-------------------|----------------------------------------|\n",
    "| Action Space      | Discrete(3)                            |\n",
    "| Observation Shape | (6,)                                   |\n",
    "| Observation High  | [1 1 1 1 12.566371 28.274334]          |\n",
    "| Observation Low   | [-1 -1 -1 -1 -12.566371 -28.274334]    |\n",
    "| Import            | `gym.make(\"Acrobot-v1\")`               |\n",
    "\n",
    "## Description of Environment\n",
    "\n",
    "The Acrobot environment consists of a two-link structure, with the joint between the two links actuated. Initially, both links hang downwards. The goal is to apply torques on the actuated joint to swing the free end of the outer-link above a line that is a certain height above the base.\n",
    "\n",
    "## Action Space\n",
    "The action is the torque applied to the joint connecting the two links.\n",
    "\n",
    "| Action  | Result                | Unit        |\n",
    "|---------|-----------------------|-------------|\n",
    "| 0       | Apply -1 torque       | Torque (N m)|\n",
    "| 1       | Apply 0 torque        | Torque (N m)|\n",
    "| 2       | Apply 1 torque        | Torque (N m)|\n",
    "\n",
    "## Observation Space\n",
    "The observation provides information about the two joint angles and their angular velocities.\n",
    "\n",
    "| Observation                | Min                 | Max              |\n",
    "|----------------------------|---------------------|------------------|\n",
    "| Cosine of theta1           | -1                  | 1                |\n",
    "| Sine of theta1             | -1                  | 1                |\n",
    "| Cosine of theta2           | -1                  | 1                |\n",
    "| Sine of theta2             | -1                  | 1                |\n",
    "| Angular velocity of theta1 | ~ -12.567 (-4 * pi) | ~ 12.567 (4 * pi)|\n",
    "| Angular velocity of theta2 | ~ -28.274 (-9 * pi) | ~ 28.274 (9 * pi)|\n",
    "\n",
    "Where:\n",
    "\n",
    "- theta1 is the angle of the first joint, where an angle of 0 indicates the first link is pointing directly downwards.\n",
    "- theta2 is relative to the angle of the first link. An angle of 0 corresponds to having the same angle between the two links.\n",
    "\n",
    "## Rewards\n",
    "All steps that do not reach the goal incur a reward of -1. Reaching the goal results in termination with a reward of 0. The reward threshold is -100.\n",
    "\n",
    "## Starting State\n",
    "Each parameter in the state (theta1, theta2, and the two angular velocities) is initialized uniformly between -0.1 and 0.1. This implies that both links are pointing downwards with some initial stochasticity.\n",
    "\n",
    "## Episode Termination\n",
    "The episode ends if:\n",
    "\n",
    "1. The free end reaches the target height, which is defined by: -cos(theta1) - cos(theta2 + theta1) > 1.0.\n",
    "2. The episode length is greater than 500 steps.\n",
    "\n",
    "---\n",
    "\n",
    "## Applying Normal DQN\n",
    "First we're going to train an agent implementing the basic DQN algorithm described by Mihn et al at DeepMind in 2013. This algorithm is explained in detail in my Lunar Lander notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef5424b9",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-07-01T20:43:18.288679Z",
     "iopub.status.busy": "2023-07-01T20:43:18.288235Z",
     "iopub.status.idle": "2023-07-01T20:51:05.466806Z",
     "shell.execute_reply": "2023-07-01T20:51:05.465075Z"
    },
    "papermill": {
     "duration": 467.195819,
     "end_time": "2023-07-01T20:51:05.469561",
     "exception": false,
     "start_time": "2023-07-01T20:43:18.273742",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: -470.42\n",
      "Episode 200\tAverage Score: -397.54\n",
      "Episode 300\tAverage Score: -331.96\n",
      "Episode 400\tAverage Score: -202.76\n",
      "Episode 500\tAverage Score: -168.72\n",
      "Episode 600\tAverage Score: -128.01\n",
      "Episode 700\tAverage Score: -109.62\n",
      "Episode 800\tAverage Score: -97.24\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class DQN(torch.nn.Module):\n",
    "    '''\n",
    "    This class defines a deep Q-network (DQN), a type of artificial neural network used in reinforcement learning.\n",
    "    The DQN is used to estimate the Q-values, which represent the expected return for each action in each state.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    state_size: int, default=6\n",
    "        The size of the state space.\n",
    "    action_size: int, default=4\n",
    "        The size of the action space.\n",
    "    hidden_size: int, default=64\n",
    "        The size of the hidden layers in the network.\n",
    "    '''\n",
    "    def __init__(self, state_size=6, action_size=3, hidden_size=64):\n",
    "        '''\n",
    "        Initialize a network with the following architecture:\n",
    "            Input layer (state_size, hidden_size)\n",
    "            Hidden layer 1 (hidden_size, hidden_size)\n",
    "            Output layer (hidden_size, action_size)\n",
    "        '''\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = torch.nn.Linear(state_size, hidden_size)\n",
    "        self.layer2 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.layer3 = torch.nn.Linear(hidden_size, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        '''\n",
    "        Define the forward pass of the DQN. This function is called when the network is called to estimate Q-values.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        state: torch.Tensor\n",
    "            The state for which to estimate the Q-values.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The estimated Q-values for each action in the input state.\n",
    "        '''\n",
    "        x = torch.relu(self.layer1(state))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        return self.layer3(x)\n",
    "    \n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "class ReplayBuffer:\n",
    "    '''\n",
    "    This class represents a replay buffer, a type of data structure commonly used in reinforcement learning algorithms.\n",
    "    The buffer stores past experiences in the environment, allowing the agent to sample and learn from them at later times.\n",
    "    This helps to break the correlation of sequential observations and stabilize the learning process.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    buffer_size: int, default=10000\n",
    "        The maximum number of experiences that can be stored in the buffer.\n",
    "    '''\n",
    "    def __init__(self, buffer_size=10000):\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        '''\n",
    "        Add a new experience to the buffer. Each experience is a tuple containing a state, action, reward,\n",
    "        the resulting next state, and a done flag indicating whether the episode has ended.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        state: array-like\n",
    "            The state of the environment before taking the action.\n",
    "        action: int\n",
    "            The action taken by the agent.\n",
    "        reward: float\n",
    "            The reward received after taking the action.\n",
    "        next_state: array-like\n",
    "            The state of the environment after taking the action.\n",
    "        done: bool\n",
    "            A flag indicating whether the episode has ended after taking the action.\n",
    "        '''\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        '''\n",
    "        Randomly sample a batch of experiences from the buffer. The batch size must be smaller or equal to the current number of experiences in the buffer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size: int\n",
    "            The number of experiences to sample from the buffer.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple of numpy.ndarray\n",
    "            A tuple containing arrays of states, actions, rewards, next states, and done flags.\n",
    "        '''\n",
    "        states, actions, rewards, next_states, dones = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.stack(states), actions, rewards, np.stack(next_states), dones\n",
    "\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Get the current number of experiences in the buffer.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            The number of experiences in the buffer.\n",
    "        '''\n",
    "        return len(self.buffer)\n",
    "    \n",
    "class DQNAgent:\n",
    "    '''\n",
    "    This class represents a Deep Q-Learning agent that uses a Deep Q-Network (DQN) and a replay memory to interact \n",
    "    with its environment.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    state_size: int, default=8\n",
    "        The size of the state space.\n",
    "    action_size: int, default=4\n",
    "        The size of the action space.\n",
    "    hidden_size: int, default=64\n",
    "        The size of the hidden layers in the network.\n",
    "    learning_rate: float, default=1e-3\n",
    "        The learning rate for the optimizer.\n",
    "    gamma: float, default=0.99\n",
    "        The discount factor for future rewards.\n",
    "    buffer_size: int, default=10000\n",
    "        The maximum size of the replay memory.\n",
    "    batch_size: int, default=64\n",
    "        The batch size for learning from the replay memory.\n",
    "    '''\n",
    "    def __init__(self, state_size=6, action_size=3, hidden_size=64, \n",
    "                 learning_rate=1e-3, gamma=0.99, buffer_size=10000, batch_size=64):\n",
    "        # Select device to train on (if CUDA available, use it, otherwise use CPU)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Discount factor for future rewards\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Batch size for sampling from the replay memory\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Number of possible actions\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # Initialize the Q-Network and Target Network with the given state size, action size and hidden layer size\n",
    "        # Move the networks to the selected device\n",
    "        self.q_network = DQN(state_size, action_size, hidden_size).to(self.device)\n",
    "        self.target_network = DQN(state_size, action_size, hidden_size).to(self.device)\n",
    "        \n",
    "        # Set weights of target network to be the same as those of the q network\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        # Set target network to evaluation mode\n",
    "        self.target_network.eval()\n",
    "\n",
    "        # Initialize the optimizer for updating the Q-Network's parameters\n",
    "        self.optimizer = torch.optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Initialize the replay memory\n",
    "        self.memory = ReplayBuffer(buffer_size)\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        '''\n",
    "        Perform a step in the environment, store the experience in the replay memory and potentially update the Q-network.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        state: array-like\n",
    "            The current state of the environment.\n",
    "        action: int\n",
    "            The action taken by the agent.\n",
    "        reward: float\n",
    "            The reward received after taking the action.\n",
    "        next_state: array-like\n",
    "            The state of the environment after taking the action.\n",
    "        done: bool\n",
    "            A flag indicating whether the episode has ended after taking the action.\n",
    "        '''\n",
    "        # Store the experience in memory\n",
    "        self.memory.push(state, action, reward, next_state, done)\n",
    "        \n",
    "        # If there are enough experiences in memory, perform a learning step\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            self.update_model()\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        '''\n",
    "        Choose an action based on the current state and the epsilon-greedy policy.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        state: array-like\n",
    "            The current state of the environment.\n",
    "        eps: float, default=0.\n",
    "            The epsilon for the epsilon-greedy policy. With probability eps, a random action is chosen.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            The chosen action.\n",
    "        '''\n",
    "        # If a randomly chosen value is greater than eps\n",
    "        if random.random() > eps:  \n",
    "            # Convert state to a PyTorch tensor and set network to evaluation mode\n",
    "            state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)  \n",
    "            self.q_network.eval()  \n",
    "\n",
    "            # With no gradient updates, get the action values from the DQN\n",
    "            with torch.no_grad():\n",
    "                action_values = self.q_network(state)\n",
    "\n",
    "            # Revert to training mode and return action\n",
    "            self.q_network.train() \n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            # Return a random action for random value > eps\n",
    "            return random.choice(np.arange(self.action_size))  \n",
    "        \n",
    "    def update_model(self):\n",
    "        '''\n",
    "        Update the Q-network based on a batch of experiences from the replay memory.\n",
    "        '''\n",
    "        # Sample a batch of experiences from memory\n",
    "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
    "\n",
    "        # Convert numpy arrays to PyTorch tensors\n",
    "        states = torch.from_numpy(states).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.array(actions)).long().to(self.device)\n",
    "        rewards = torch.from_numpy(np.array(rewards)).float().to(self.device)\n",
    "        next_states = torch.from_numpy(next_states).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.array(dones).astype(np.uint8)).float().to(self.device)\n",
    "\n",
    "        # Get Q-values for the actions that were actually taken\n",
    "        q_values = self.q_network(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "        \n",
    "        # Get maximum Q-value for the next states from target network\n",
    "        next_q_values = self.target_network(next_states).max(1)[0].detach()\n",
    "        \n",
    "        # Compute the expected Q-values\n",
    "        expected_q_values = rewards + self.gamma * next_q_values * (1 - dones)\n",
    "\n",
    "        # Compute the loss between the current and expected Q values\n",
    "        loss = torch.nn.MSELoss()(q_values, expected_q_values)\n",
    "        \n",
    "        # Zero all gradients\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # Backpropagate the loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # Step the optimizer\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        '''\n",
    "        Update the weights of the target network to match those of the Q-network.\n",
    "        '''\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "def train(agent, env, n_episodes=2000, eps_start=1.0, eps_end=0.01, eps_decay=0.995, target_update=10):\n",
    "    '''\n",
    "    Train a DQN agent.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    agent: DQNAgent\n",
    "        The agent to be trained.\n",
    "    env: gym.Env\n",
    "        The environment in which the agent is trained.\n",
    "    n_episodes: int, default=2000\n",
    "        The number of episodes for which to train the agent.\n",
    "    eps_start: float, default=1.0\n",
    "        The starting epsilon for epsilon-greedy action selection.\n",
    "    eps_end: float, default=0.01\n",
    "        The minimum value that epsilon can reach.\n",
    "    eps_decay: float, default=0.995\n",
    "        The decay rate for epsilon after each episode.\n",
    "    target_update: int, default=10\n",
    "        The frequency (number of episodes) with which the target network should be updated.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    list of float\n",
    "        The total reward obtained in each episode.\n",
    "    '''\n",
    "\n",
    "    # Initialize the scores list and scores window\n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    eps = eps_start\n",
    "\n",
    "    # Loop over episodes\n",
    "    for i_episode in range(1, n_episodes + 1):\n",
    "        \n",
    "        # Reset environment and score at the start of each episode\n",
    "        state, _ = env.reset()\n",
    "        score = 0 \n",
    "\n",
    "        # Loop over steps\n",
    "        while True:\n",
    "            \n",
    "            # Select an action using current agent policy then apply in environment\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action) \n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Update the agent, state and score\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state \n",
    "            score += reward\n",
    "\n",
    "            # End the episode if done\n",
    "            if done:\n",
    "                break \n",
    "        \n",
    "        # At the end of episode append and save scores\n",
    "        scores_window.append(score)\n",
    "        scores.append(score) \n",
    "\n",
    "        # Decrease epsilon\n",
    "        eps = max(eps_end, eps_decay * eps)\n",
    "\n",
    "        # Print some info\n",
    "        print(f\"\\rEpisode {i_episode}\\tAverage Score: {np.mean(scores_window):.2f}\", end=\"\")\n",
    "\n",
    "        # Update target network every target_update episodes\n",
    "        if i_episode % target_update == 0:\n",
    "            agent.update_target_network()\n",
    "            \n",
    "        # Print average score every 100 episodes\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "\n",
    "        # This environment is considered to be solved for a mean score of -100 or greater, so stop training.\n",
    "        if i_episode % 100 == 0 and np.mean(scores_window) >= -100:\n",
    "            break\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "# Make an environment\n",
    "env = gym.make('Acrobot-v1')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Initilize a DQN agent\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "# Train it\n",
    "scores = train(agent, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ae6c88",
   "metadata": {
    "papermill": {
     "duration": 0.082525,
     "end_time": "2023-07-01T20:51:05.633702",
     "exception": false,
     "start_time": "2023-07-01T20:51:05.551177",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "![](https://i.imgur.com/jglC8zX.gif)\n",
    "\n",
    "## N-Step DQN\n",
    "N-step DQN is an improvement to Q-learning first introduced by Sutton in 1988 in Learning to Predict by the Methods of Temporal Differences.\n",
    "\n",
    "To get the idea, let's look at the Bellman update used in Q-learning:\n",
    "\n",
    "$$ Q(s_t, a_t) = r_t + \\gamma \\cdot max_aQ(s_{t+1}, a_{t+1}) $$\n",
    "\n",
    "This equation is recursive, which means that we can express $Q(s_{t+1}, a_{t+1})$ in terms of itself, which gives us:\n",
    "\n",
    "$$ Q(s_t, a_t) = r_t + \\gamma \\cdot max_a[r_{a, t+1} + \\gamma \\cdot max_{a'}Q(s_{t+2}, a')] $$\n",
    "\n",
    "Value ra,t+1 means local reward at time t+1, after issuing action a. However, if we assume that action a at the step t+1 was chosen optimally, or close to optimally, we can omit the max operation and obtain this:\n",
    "\n",
    "$$ Q(s_t, a_t) = r_t + \\gamma r_{t+1} + \\gamma^2max_{a'}Q(S_{t+2}, a') $$\n",
    "\n",
    "This value can be unrolled again and again any number of times. As you may\n",
    "guess, this unrolling can be easily applied to our DQN update by replacing one-step transition sampling with longer transition sequences of n-steps. \n",
    "\n",
    "### Implementation\n",
    "\n",
    "1. **Initialization**: Begin by initializing the parameters for two neural networks, $Q(s,a)$ (referred to as the online network) and $\\hat{Q}(s,a)$ (known as the target network), with random weights. Both networks serve the function of mapping a state-action pair to a Q-value, which is an estimate of the expected return from that pair. Also, set the exploration probability $\\epsilon$ to 1.0, and create an empty replay buffer to store past transition experiences.\n",
    "\n",
    "2. **Action Selection**: Utilize an epsilon-greedy strategy for action selection. With a probability of $\\epsilon$, select a random action $a$, but in all other instances, choose the action $a$ that maximizes the Q-value, i.e., $a = argmax_aQ(s,a)$.\n",
    "\n",
    "3. **Experience Collection**: Execute the chosen action $a$ within the environment emulator and observe the resulting immediate reward $r$ and the next state $s'$.\n",
    "\n",
    "4. **Experience Storage**: Store the transition $(s,a,r,s')$ in the replay buffer for future reference. But instead of storing individual transitions, you store the last n steps as one experience $(s,a,R,s')$ where $R$ is the discounted sum of the rewards over the n steps. If you haven't completed n steps yet, then you estimate $R$ using the value of the next state $s'$ from the online network as an approximation for the remaining steps.\n",
    "\n",
    "5. **Sampling**: Randomly sample a mini-batch of transitions from the replay buffer for training the online network.\n",
    "\n",
    "6. **Target Computation**: For every transition in the sampled mini-batch, compute the target value $y$. If the episode has ended at this step, $y$ is simply the reward $R$. Otherwise, $y$ is the sum of the reward $R$ and the discounted estimated optimal future Q-value for the remaining steps, i.e., $y = R + \\gamma^{n} \\max_{a' \\in A} \\hat{Q}(s', a')$\n",
    "\n",
    "7. **Loss Calculation**: Compute the loss, which is the squared difference between the Q-value predicted by the online network and the computed target, i.e., $\\mathcal{L} = (Q(s,a) - y)^2$\n",
    "\n",
    "8. **Online Network Update**: Update the parameters of the online network $Q(s,a)$ using Stochastic Gradient Descent (SGD) to minimize the loss.\n",
    "\n",
    "9. **Target Network Update**: Every $N$ steps, update the target network by copying the weights from the online network to the target network $\\hat{Q}(s,a)$.\n",
    "\n",
    "Iterate: Repeat the process from step 2 until convergence.\n",
    "\n",
    "This extension allows the DQN to take into account a longer trajectory in the calculation of the target Q-value, potentially leading to faster and more stable learning. The parameter n is a hyperparameter and can be tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4565e3c6",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-07-01T20:51:05.808482Z",
     "iopub.status.busy": "2023-07-01T20:51:05.807711Z",
     "iopub.status.idle": "2023-07-01T20:51:05.837862Z",
     "shell.execute_reply": "2023-07-01T20:51:05.836627Z"
    },
    "papermill": {
     "duration": 0.127278,
     "end_time": "2023-07-01T20:51:05.840732",
     "exception": false,
     "start_time": "2023-07-01T20:51:05.713454",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NStepDQNAgent:\n",
    "    '''\n",
    "    This class is the same as the DQN class above with a few modifications.\n",
    "    Changes have been highlighted.\n",
    "\n",
    "    New Parameters\n",
    "    ----------\n",
    "    n_steps (int)\n",
    "        The number of steps for n-step DQN\n",
    "    '''\n",
    "    def __init__(self, state_size=6, action_size=3, hidden_size=64, \n",
    "                 learning_rate=1e-3, gamma=0.99, buffer_size=10000, batch_size=64,\n",
    "                 n_steps=2):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.action_size = action_size\n",
    "        self.q_network = DQN(state_size, action_size, hidden_size).to(self.device)\n",
    "        self.target_network = DQN(state_size, action_size, hidden_size).to(self.device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.target_network.eval()\n",
    "        self.optimizer = torch.optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
    "        self.memory = ReplayBuffer(buffer_size)\n",
    "\n",
    "        # Number of steps for n-step DQN\n",
    "        self.n_steps = n_steps\n",
    "        # Temporary buffer for n-step transitions\n",
    "        # We need this as we need to delay pushing transitions into our main ReplayBuffer\n",
    "        # until we've seen n steps into the future. This is so we can compute the n-step return\n",
    "        self.n_step_buffer = deque(maxlen=n_steps)\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        # Store experience to n-step buffer\n",
    "        self.n_step_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "        # If our step_buffer is filled\n",
    "        if len(self.n_step_buffer) == self.n_steps:\n",
    "            self.push_to_memory()\n",
    "\n",
    "        if done:\n",
    "            while len(self.n_step_buffer) > 0:\n",
    "                self.push_to_memory()\n",
    "\n",
    "        # If we have enough experiences, learn, as in regular DQN\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            self.update_model()\n",
    "\n",
    "    def push_to_memory(self):\n",
    "        # We format our transitions as (s, a, R, s')\n",
    "            # Where:\n",
    "            #   s is our starting state\n",
    "            #   a is the first action we took in that state\n",
    "            #   R is the total reward for taking a in s and a' in st+1 (or more for higher n_steps)\n",
    "            #   s' is the final state we end up in after n steps\n",
    "        state_0, action_0, _, _, _ = self.n_step_buffer[0]\n",
    "        _, _, _, next_state_n, done_n = self.n_step_buffer[-1]\n",
    "\n",
    "        # Sum of discount factor to the power of the step multiplied by reward for that step\n",
    "        reward_n = sum(self.gamma**i * trans[2] for i, trans in enumerate(list(self.n_step_buffer)))\n",
    "        # Push to our replay buffer\n",
    "        self.memory.push(state_0, action_0, reward_n, next_state_n, done_n)\n",
    "        # Pop the oldest experience out\n",
    "        self.n_step_buffer.popleft()\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        if random.random() > eps:  \n",
    "            state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)  \n",
    "            self.q_network.eval()  \n",
    "\n",
    "            with torch.no_grad():\n",
    "                action_values = self.q_network(state)\n",
    "\n",
    "            self.q_network.train() \n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))  \n",
    "        \n",
    "    def update_model(self):\n",
    "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
    "        states = torch.from_numpy(states).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.array(actions)).long().to(self.device)\n",
    "        rewards = torch.from_numpy(np.array(rewards)).float().to(self.device)\n",
    "        next_states = torch.from_numpy(next_states).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.array(dones).astype(np.uint8)).float().to(self.device)\n",
    "        q_values = self.q_network(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "        next_q_values = self.target_network(next_states).max(1)[0].detach()\n",
    "        \n",
    "        # Compute the expected Q-values using the unrolled Bellman equation\n",
    "        expected_q_values = rewards + self.gamma ** self.n_steps * next_q_values * (1 - dones)\n",
    "\n",
    "        loss = torch.nn.MSELoss()(q_values, expected_q_values)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a98e42",
   "metadata": {
    "papermill": {
     "duration": 0.08081,
     "end_time": "2023-07-01T20:51:06.002390",
     "exception": false,
     "start_time": "2023-07-01T20:51:05.921580",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### N-Step DQN with n_steps = 2\n",
    "Let's train our N-Steps DQN agent with n_steps=2. This agent should converge faster than the classic DQN implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08ccae55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-01T20:51:06.167096Z",
     "iopub.status.busy": "2023-07-01T20:51:06.166689Z",
     "iopub.status.idle": "2023-07-01T20:55:28.116898Z",
     "shell.execute_reply": "2023-07-01T20:55:28.115419Z"
    },
    "papermill": {
     "duration": 262.035483,
     "end_time": "2023-07-01T20:55:28.119575",
     "exception": false,
     "start_time": "2023-07-01T20:51:06.084092",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: -474.68\n",
      "Episode 200\tAverage Score: -242.26\n",
      "Episode 300\tAverage Score: -166.71\n",
      "Episode 400\tAverage Score: -129.05\n",
      "Episode 500\tAverage Score: -95.16\n"
     ]
    }
   ],
   "source": [
    "# Make an environment\n",
    "env = gym.make('Acrobot-v1')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Initilize a DQN agent\n",
    "agent = NStepDQNAgent(state_size, action_size, n_steps=2)\n",
    "# Train it\n",
    "scores = train(agent, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451fdb72",
   "metadata": {
    "papermill": {
     "duration": 0.122384,
     "end_time": "2023-07-01T20:55:28.365101",
     "exception": false,
     "start_time": "2023-07-01T20:55:28.242717",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "![](https://i.imgur.com/Zxcu6gs.gif)\n",
    "\n",
    "#### Observations:\n",
    "- 2-step DQN converges significantly faster than the regular DQN algorithm.\n",
    "- Let's see if we can increase convergence speed even further with n_step=3 and n_steps=4\n",
    "\n",
    "### n_steps = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16741b9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-01T20:55:28.613105Z",
     "iopub.status.busy": "2023-07-01T20:55:28.611803Z",
     "iopub.status.idle": "2023-07-01T20:59:43.614537Z",
     "shell.execute_reply": "2023-07-01T20:59:43.613420Z"
    },
    "papermill": {
     "duration": 255.132307,
     "end_time": "2023-07-01T20:59:43.620033",
     "exception": false,
     "start_time": "2023-07-01T20:55:28.487726",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: -479.05\n",
      "Episode 200\tAverage Score: -244.98\n",
      "Episode 300\tAverage Score: -152.49\n",
      "Episode 400\tAverage Score: -103.61\n",
      "Episode 500\tAverage Score: -98.41\n"
     ]
    }
   ],
   "source": [
    "# Make an environment\n",
    "env = gym.make('Acrobot-v1')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Initilize a DQN agent\n",
    "agent = NStepDQNAgent(state_size, action_size, n_steps=3)\n",
    "\n",
    "# Train it\n",
    "scores = train(agent, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2faad91",
   "metadata": {
    "papermill": {
     "duration": 0.173526,
     "end_time": "2023-07-01T20:59:44.058101",
     "exception": false,
     "start_time": "2023-07-01T20:59:43.884575",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "![](https://i.imgur.com/2Ubr9D9.gif)\n",
    "\n",
    "#### n_steps = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60852dc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-01T20:59:44.399497Z",
     "iopub.status.busy": "2023-07-01T20:59:44.399086Z",
     "iopub.status.idle": "2023-07-01T21:04:26.018752Z",
     "shell.execute_reply": "2023-07-01T21:04:26.017492Z"
    },
    "papermill": {
     "duration": 281.793138,
     "end_time": "2023-07-01T21:04:26.021450",
     "exception": false,
     "start_time": "2023-07-01T20:59:44.228312",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: -447.80\n",
      "Episode 200\tAverage Score: -289.24\n",
      "Episode 300\tAverage Score: -143.01\n",
      "Episode 400\tAverage Score: -103.43\n",
      "Episode 500\tAverage Score: -101.58\n",
      "Episode 600\tAverage Score: -97.76\n"
     ]
    }
   ],
   "source": [
    "# Make an environment\n",
    "env = gym.make('Acrobot-v1')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Initilize a DQN agent\n",
    "agent = NStepDQNAgent(state_size, action_size, n_steps=4)\n",
    "\n",
    "# Train it\n",
    "scores = train(agent, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdd6edd",
   "metadata": {
    "papermill": {
     "duration": 0.213951,
     "end_time": "2023-07-01T21:04:26.451256",
     "exception": false,
     "start_time": "2023-07-01T21:04:26.237305",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "![](https://i.imgur.com/Ef2aTFR.gif)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1361.346898,
   "end_time": "2023-07-01T21:04:28.193677",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-07-01T20:41:46.846779",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
