{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Modern PPO on Atari Environments\n### Objective\n\nThe aim of this notebook is to construct a Proximal Policy Optimization (PPO) algorithm tailored for Atari environments – classic arcade games converted into test beds for reinforcement learning algorithms. Unlike pre-packaged solutions, building PPO from scratch provides complete control over every component, permits tailored adjustments, streamlined debugging, and detailed visualization of the training process.\n\n### Methodology\n1. **Initialization**: Configure the environment, define the neural network architecture, and allocate necessary storage tensors on the GPU.\n2. **Rollouts**: Generate batches of experiences from multiple parallel environments following the current policy.\n3. **Advantage estimation**: Compute advantages using Generalized Advantage Estimation (GAE) or traditional methods, based on collected rewards and states.\n4. **Optimization**: Conduct multiple epochs of mini-batch updates employing clipped value function objectives and entropy regularization.\n5. **Logging and visualization**: Integrate Tensorboard to track and visualize key metrics and performance indicators, offering detailed insight into the training dynamics.\n\nThis implementation offers performance that is competitive with established implementations like OpenAI’s Stable Baselines, while providing the flexibility to add or remove contemporary modifications.\n\n<img src=\"https://i.imgur.com/ObjphNJ.gif\" \n     align=\"Center\" \n     width=\"600\" />\n\n### Required Packages\nThese packages are by default not installed on Kaggle, so we will quickly install them first.","metadata":{}},{"cell_type":"code","source":"!pip install gym[atari,accept-rom-license] --quiet\n!pip install ale-py --quiet","metadata":{"execution":{"iopub.status.busy":"2023-09-09T13:28:03.197160Z","iopub.execute_input":"2023-09-09T13:28:03.197629Z","iopub.status.idle":"2023-09-09T13:28:49.450803Z","shell.execute_reply.started":"2023-09-09T13:28:03.197589Z","shell.execute_reply":"2023-09-09T13:28:49.449360Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### Configuration\nThis code sets up the configuration for running a PPO experiment on an Atari Breakout environment. \n\nThe `config_dict` contains hyperparameters and settings like the learning rate, number of environments, and discount factor. The `Config` class converts this dictionary into an object for easier access throughout the code.","metadata":{}},{"cell_type":"code","source":"import torch\nimport time\nimport numpy as np\nimport gym\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.tensorboard import SummaryWriter\n\nconfig_dict = {\n    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n    'run_name': \"breakout-experiment-1\", # Name of the experiment/run for logging\n    'gym_id': \"BreakoutNoFrameskip-v4\",  # Name of Atari Gym environment\n\n    'total_timesteps': 10000000,   # Total number of training steps\n    'n_envs': 8,                   # Total number of vectorized environments\n    'n_steps': 128,                # Number of steps performed in each environment for each rollout\n    'n_minibatches': 4,            # Number of minibatches training batch is split into\n    'update_epochs': 4,            # Number of full learning steps\n    'frame_skip': 4,               # Number of frames to skip in Atari environment\n    'hidden_size': 64,             # Number of neurons in actor and critic network hidden layers\n    'learning_rate': 2.5e-4,       # Optimizer learning rate\n    'anneal_lr': True,             # Toggle learning rate annealing\n    'gamma': 0.99,                 # Discount factor\n    'gae': True,                   # Toggle general advantage estimation\n    'gae_lambda': 0.95,            # Lambda value used in gae calculation\n    'clip_coef': 0.1,              # Amount of policy clipping\n    'norm_advantages': True,       # Toggle advantage normalization\n    'clip_value_loss': True,       # Toggle value loss clipping\n    'weight_value_loss': 0.5,      # Weight of value loss relative to policy loss\n    'weight_ent_loss': 0.01,       # Entropy loss weight\n    'max_grad_norm': 0.5           # Global gradient clipping max norm\n}\n\nconfig_dict['batch_size'] = int(config_dict['n_envs'] * config_dict['n_steps'])\nconfig_dict['minibatch_size'] = int(config_dict['batch_size'] // config_dict['n_minibatches'])\n\n# Convert to a struct esque structure\nclass Config:\n    def __init__(self, dictionary):\n        for key, value in dictionary.items():\n            setattr(self, key, value)\n\nconfig = Config(config_dict)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-09T13:35:48.518350Z","iopub.execute_input":"2023-09-09T13:35:48.518831Z","iopub.status.idle":"2023-09-09T13:36:01.925430Z","shell.execute_reply.started":"2023-09-09T13:35:48.518789Z","shell.execute_reply":"2023-09-09T13:36:01.924270Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### Initialize Tensorboard\nHere, we set up Tensorboard for logging and visualization. \n\nWe initialize a `SummaryWriter` object, which will write log files to a specified directory. The hyperparameters from the configuration are also recorded as text data in Tensorboard for easy reference during the experiment's evaluation.","metadata":{}},{"cell_type":"code","source":"# Set name and initialize writer\nrun_name = config.run_name\nwriter = SummaryWriter(f\"logs/{run_name}\")\n\n# Record hyperparameter settings\nwriter.add_text(\n    \"Hyperparameters\",\n    \"| Param | Value |\\n| ----- | ----- |\\n%s\" % (\"\\n\".join([f\"| {key} | {value} |\" for key, value in config_dict.items()]))\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T13:36:01.927375Z","iopub.execute_input":"2023-09-09T13:36:01.928659Z","iopub.status.idle":"2023-09-09T13:36:01.939214Z","shell.execute_reply.started":"2023-09-09T13:36:01.928609Z","shell.execute_reply":"2023-09-09T13:36:01.937930Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### Wrappers and Vectorized Environments\nNext, we apply various environment wrappers and then vectorize these environments for parallel execution. \n\nWrappers like `FireResetEnv` and `ClipRewardEnv` modify the default environment behavior; the former ensures that the 'FIRE' action is taken immediately after resets, while the latter clips rewards to -1, 0, or +1 based on their sign. \n\nThe `AtariWrappers` function applies several standard preprocessing steps like frame skipping, resizing the observation space, and grayscaling the frames. \n\nThe `make_env` function combines these wrappers and returns a callable environment factory. Finally, `gym.vector.SyncVectorEnv` is used to parallelize these wrapped environments, enabling faster data collection.","metadata":{}},{"cell_type":"code","source":"# This wrapper is an exact copy of the SB3 wrapper\nclass FireResetEnv(gym.Wrapper):\n    \"\"\"\n    Take action on reset for environments that are fixed until firing.\n    Args:\n        env (gym.Env): The environment to wrap\n    \"\"\"\n    def __init__(self, env: gym.Env):\n        gym.Wrapper.__init__(self, env)\n        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n        assert len(env.unwrapped.get_action_meanings()) >= 3\n\n    def reset(self, **kwargs) -> np.ndarray:\n        self.env.reset(**kwargs)\n        obs, _, terminated, truncated, info = self.env.step(1)\n        if terminated or truncated:\n            self.env.reset(**kwargs)\n        obs, _, terminated, truncated, info = self.env.step(2)\n        if terminated or truncated:\n            self.env.reset(**kwargs)\n        return obs, info\n\n# This wrapper is an exact copy of the SB3 wrapper\nclass ClipRewardEnv(gym.RewardWrapper):\n    \"\"\"\n    Clips the reward to {+1, 0, -1} by its sign.\n    Args:\n        env (gym.Env): The environment to wrap\n    \"\"\"\n\n    def __init__(self, env: gym.Env):\n        gym.RewardWrapper.__init__(self, env)\n    \n    def reward(self, reward: float) -> float:\n        return np.sign(reward)\n\ndef AtariWrappers(env):\n    env = gym.wrappers.AtariPreprocessing(\n        env,\n        noop_max=30,                   # 30 random actions a the beginning of an episode\n        frame_skip=config.frame_skip,  # Repeats each input 3 times\n        screen_size=84,                # Changes observation size to 84x84\n        terminal_on_life_loss=True,    # Returns done=True if episode terminates\n        grayscale_obs=True,            # Convert RGB to grayscale\n        scale_obs=True,                # Scales observations to range 0-1\n    )\n    return env\n\ndef make_env(gym_id, idx, run_name):\n    def thunk():\n        env = gym.make(gym_id, render_mode='rgb_array')\n        env = AtariWrappers(env)                           # Use preconfigured Atari preprocessing wrapper\n        if 'FIRE' in env.unwrapped.get_action_meanings():  # Automatically 'fire' at the start\n            env = FireResetEnv(env)                        \n        env = ClipRewardEnv(env)                           # Clip all rewards to {-1, 0, +1}\n        env = gym.wrappers.FrameStack(env, 4)              # Use stacks of 4 frames for each observation\n        return env\n    return thunk\n\nenvs = gym.vector.SyncVectorEnv([\n    make_env(config.gym_id,\n              i, \n              config.run_name) for i in range(config.n_envs)\n    ])","metadata":{"execution":{"iopub.status.busy":"2023-09-09T13:36:04.994435Z","iopub.execute_input":"2023-09-09T13:36:04.995125Z","iopub.status.idle":"2023-09-09T13:36:06.840025Z","shell.execute_reply.started":"2023-09-09T13:36:04.995090Z","shell.execute_reply":"2023-09-09T13:36:06.838933Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n[Powered by Stella]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Network Architecture and Layer Initialization\nThe code defines our policy and value neural network using PyTorch. The network consists of a feature extraction base made up of convolutional layers followed by two heads: one for the policy (actor) and one for the value function (critic). We use ReLU activation functions after each convolutional layer.\n\nThe network is initialized using orthogonal initialization for the weights and constant initialization for the biases, which is explicitly conducted using the `layer_init` function. Standard deviations are specified for different parts of the network: 1 for the critic and 0.01 for the actor.\n\nThe forward method returns four outputs: the selected action, the log probability of that action, the entropy of the action distribution, and the estimated state value.  Having all of these values computed during a single forward pass improves computational efficiency. The `forward_critic` and `select_action` methods are utility functions that provide a more specific output based on the context.\n\nFinally, an Adam optimizer is configured with the learning rate and modified epsilon value, as the default Adam epsilon value is often found to be unstable.","metadata":{}},{"cell_type":"code","source":"def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n    nn.init.orthogonal_(layer.weight, std)\n    nn.init.constant_(layer.bias, bias_const)\n    return layer\n\nclass PPONetwork(nn.Module):\n    def __init__(self, input_shape, n_actions, hidden_size=512):\n        super().__init__()\n\n        self.base = nn.Sequential(\n            layer_init(nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4)),\n            nn.ReLU(),\n            layer_init(nn.Conv2d(32, 64, kernel_size=4, stride=2)),\n            nn.ReLU(),\n            layer_init(nn.Conv2d(64, 64, kernel_size=3, stride=1)),\n            nn.ReLU(),\n            nn.Flatten(),\n            layer_init(nn.Linear(64 * 7 * 7, hidden_size)),\n            nn.ReLU()\n        )\n\n        self.actor = nn.Sequential(\n            layer_init(nn.Linear(hidden_size, n_actions), std=0.01)\n        )\n\n        self.critic = nn.Sequential(\n            layer_init(nn.Linear(hidden_size, 1), std=1.)\n        )\n\n    def forward(self, x, action=None):\n        \"\"\"\n        Returns:\n            action (torch.tensor): Action predicted by agent for each state in the batch\n            log_probs (torch.tensor): The log probability of the agent taking an action following the current policy\n            entropy (torch.tensor): The entropy of each distribution in the batch\n            value (torch.tensor): The predicted value of each state in the batch using the critic network\n        \"\"\"\n        x = self.base(x)\n        probs = torch.distributions.Categorical(logits=self.actor(x))\n        if action is None:\n            action = probs.sample()\n        return action, probs.log_prob(action), probs.entropy(), self.critic(x)\n\n    def forward_critic(self, x):\n        x = self.base(x)\n        return self.critic(x)\n    \n    def select_action(self, x):\n        x = self.base(x)\n        probs = torch.distributions.Categorical(logits=self.actor(x))\n        return probs.sample()\n\n    def select_action_deterministic(self, x):\n        x = self.base(x)\n        logits = self.actor(x)\n        return torch.argmax(logits, dim=-1)\n\n\ninput_shape = envs.single_observation_space.shape\nn_actions = envs.single_action_space.n\n\nnetwork = PPONetwork(input_shape, n_actions, config.hidden_size).to(config.device)\n\n# Change Adam epsilon to 1e-5 for increased stability\noptimizer = torch.optim.Adam(network.parameters(), lr=config.learning_rate, eps=1e-5)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T13:36:09.165662Z","iopub.execute_input":"2023-09-09T13:36:09.166068Z","iopub.status.idle":"2023-09-09T13:36:09.362351Z","shell.execute_reply.started":"2023-09-09T13:36:09.166037Z","shell.execute_reply":"2023-09-09T13:36:09.361103Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### Memory\nFor efficient execution, we allocate memory for states, actions, rewards, done flags, log probabilities, and values upfront, placing them on the GPU if available.","metadata":{}},{"cell_type":"code","source":"states = torch.zeros((config.n_steps, config.n_envs) + envs.single_observation_space.shape).to(config.device)\nactions = torch.zeros((config.n_steps, config.n_envs)).to(config.device)\nrewards = torch.zeros((config.n_steps, config.n_envs)).to(config.device)\ndones = torch.zeros((config.n_steps, config.n_envs)).to(config.device)\nlogprobs = torch.zeros((config.n_steps, config.n_envs)).to(config.device)\nvalues = torch.zeros((config.n_steps, config.n_envs)).to(config.device)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T13:36:27.374464Z","iopub.execute_input":"2023-09-09T13:36:27.374868Z","iopub.status.idle":"2023-09-09T13:36:27.378769Z","shell.execute_reply.started":"2023-09-09T13:36:27.374837Z","shell.execute_reply":"2023-09-09T13:36:27.377933Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### Main Training Loop\n\nThe main training loop iterates over the total number of updates determined by `num_updates`. During each iteration, we roll out trajectories for `n_steps` for all parallel environments.\n\n1. **Annealed Learning Rate**: If `config.anneal_lr` is enabled, we decrease the learning rate linearly based on how many updates have been performed.\n   \n2. **Policy Evaluation and Action Selection**: We use the policy network to select actions and compute the value estimates without gradient computation.\n   \n3. **Environment Step**: After taking actions in the environments, we get next states and rewards. We also update episodic metrics like returns and episode lengths.\n\n4. **Advantage Estimation**: Depending on whether GAE is enabled, we compute either Generalized Advantage Estimates or simple returns. We then normalize advantages if `config.norm_advantages` is true.\n\n#### Mini-Batch Training\n\n1. **Random Shuffling**: We shuffle the batch indices before each epoch of mini-batch updates.\n\n2. **Policy and Value Update**: Within each mini-batch, we compute new log probabilities, values, and entropies for the sampled states and actions.\n\n3. **Loss Calculation**: We calculate the PPO surrogate loss, clipped value loss, and entropy loss. These are weighted and summed to form the final loss which is minimized.\n\n4. **Gradient Clipping**: After backpropagation, gradients are clipped to ensure they don't exceed a predefined threshold (`config.max_grad_norm`).\n\n#### Logging and Metrics\n\nWe log key metrics such as learning rate, value, policy, and entropy losses. We also track approximate KL divergence and clip fractions. Finally, explained variance and steps per second (SPS) are calculated and logged for performance monitoring.","metadata":{}},{"cell_type":"code","source":"# Logging and training information\nglobal_step = 0\nstart_time = time.time()\nnum_updates = config.total_timesteps // config.batch_size # Total number of learning update that will be performed\n\n# Initial state and done flag\nstate = torch.Tensor(envs.reset()[0]).to(config.device)\ndone = torch.zeros(config.n_envs).to(config.device)\n\n# Manually initialize metrics for tracking as RecordEpisodeStatistics is currently broken for vectorized environments\nepisodic_return = np.zeros([config.n_envs])\nepisode_step_count = np.zeros([config.n_envs])\n\nfor update in range(1, num_updates + 1):\n    # Annealed learning rate\n    if config.anneal_lr:\n        fraction = 1.0 - ((update - 1.0) / num_updates)\n        lr_current = fraction * config.learning_rate\n        optimizer.param_groups[0]['lr'] = lr_current\n    \n    # Rollouts\n    for step in range(0, config.n_steps):\n        global_step += 1 * config.n_envs\n        states[step] = state\n        dones[step] = done\n\n        # Action selection doesn't require gradient updates\n        with torch.no_grad():\n            action, logprob, _, value = network(state)\n            values[step] = value.flatten()\n        actions[step] = action\n        logprobs[step] = logprob\n\n        state, reward, terminated, truncated, info = envs.step(action.cpu().numpy())\n        done = np.logical_or(terminated, truncated)\n        rewards[step] = torch.tensor(reward).to(config.device).view(-1)\n        state = torch.Tensor(state).to(config.device)\n        done = torch.Tensor(done).to(config.device)\n\n        # Logging\n        episodic_return += reward\n        episode_step_count += 1\n\n        # If an episode is done\n        if 'final_observation' in info.keys():\n            for i_env, done_flag in enumerate(info['_final_observation']):\n                if done_flag:\n                    print(f\"\\rglobal_step={global_step}, episodic_return={episodic_return[i_env]}\", end='')\n\n                    writer.add_scalar('charts/episodic_return', episodic_return[i_env], global_step)\n                    writer.add_scalar('charts/episodic_length', episode_step_count[i_env], global_step)\n\n                    episodic_return[i_env], episode_step_count[i_env] = 0., 0.\n\n    # General advantage estimation\n    with torch.no_grad():\n        next_value = network.forward_critic(state).reshape(1, -1)\n        if config.gae:\n            advantages = torch.zeros_like(rewards).to(config.device)\n            lastgaelam = 0\n            for t in reversed(range(config.n_steps)):\n                if t == config.n_steps - 1:\n                    next_non_terminal = 1.0 - done\n                    next_values = next_value\n                else:\n                    next_non_terminal = 1.0 - dones[t + 1]\n                    next_values = values[t + 1]\n                delta = rewards[t] + config.gamma * next_values * next_non_terminal - values[t]\n                advantages[t] = lastgaelam = delta + config.gamma * config.gae_lambda * next_non_terminal * lastgaelam\n            returns = advantages + values\n        # Vanilla advantage calculation \n        else:\n            returns = torch.zeros_like(rewards).to(config.device)\n            for t in reversed(range(config.num_steps)):\n                if t == config.n_steps - 1:\n                    next_non_terminal = 1.0 - done\n                    next_return = next_value\n                else:\n                    next_non_terminal = 1.0 - dones[t + 1]\n                    next_return = returns[t + 1]\n                returns[t] = rewards[t] + config.gamma * next_non_terminal * next_return\n            advantages = returns - values\n\n    # Tracking metric\n    clipfracs = []\n\n    # Minibatch update\n    # Flatten batches b_ indicates a full batch, mb_ indicates a minibatch\n    b_states = states.reshape((-1,) + input_shape)\n    b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n    b_logprobs= logprobs.reshape(-1)\n    b_advantages = advantages.reshape(-1)\n    b_returns = returns.reshape(-1)\n    b_values = values.reshape(-1)\n    \n    # Optimizing the policy and value network\n    b_inds = np.arange(config.batch_size) # Batch indices\n    for epoch in range(config.update_epochs):\n        np.random.shuffle(b_inds)\n        for start in range(0, config.batch_size, config.minibatch_size):\n            end = start + config.minibatch_size\n            mb_inds = b_inds[start:end]\n\n            _, newlogprob, entropy, newvalue = network(b_states[mb_inds], b_actions.long()[mb_inds])\n            logratio = newlogprob - b_logprobs[mb_inds]\n            ratio = logratio.exp()\n\n            # Tracking metrics\n            with torch.no_grad():\n                # Calculate approx KL divergence between policies http://joschu.net/blog/kl-approx.html\n                approx_kl = ((ratio - 1) - logratio).mean()\n                # Number of triggered clips\n                clipfracs +=  [((ratio - 1.).abs() > config.clip_coef).float().mean()]\n\n            # Advantage normalization\n            mb_advantages = b_advantages[mb_inds]\n            if config.norm_advantages:\n                mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n            \n            # Clipped surrogate objective\n            loss_surrogate_unclipped = -mb_advantages * ratio\n            loss_surrogate_clipped = -mb_advantages * torch.clip(ratio, \n                                             1 - config.clip_coef, \n                                             1 + config.clip_coef)\n            loss_policy = torch.max(loss_surrogate_unclipped, loss_surrogate_clipped).mean()\n\n            # Value loss clipping\n            newvalue = newvalue.view(-1)\n            if config.clip_value_loss:\n                loss_v_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n                value_clipped = b_values[mb_inds] + torch.clip(\n                    newvalue - b_values[mb_inds],\n                    -config.clip_coef,\n                    config.clip_coef\n                )\n                loss_v_clipped = (value_clipped - b_returns[mb_inds]) ** 2 # MSE\n                loss_v_max = torch.max(loss_v_unclipped, loss_v_clipped)\n                loss_value = 0.5 * loss_v_max.mean()\n            else:\n                loss_value = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean() # MSE\n            \n            # Entropy loss\n            loss_entropy = entropy.mean()\n\n            # Weighted value loss\n            loss = loss_policy + config.weight_ent_loss * -loss_entropy + config.weight_value_loss * loss_value\n\n            # Global gradient clipping\n            optimizer.zero_grad()\n            loss.backward()\n            nn.utils.clip_grad_norm_(network.parameters(), config.max_grad_norm)\n            optimizer.step()\n\n    y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n    var_y = np.var(y_true)\n    explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n    \n    # Number of steps per second\n    sps = int(global_step / (time.time() - start_time))\n    mean_clipfracs = np.mean([item.cpu().numpy() for item in clipfracs])\n\n    # Write all metrics to Tensorboard\n    writer.add_scalar('charts/learning_rate', optimizer.param_groups[0]['lr'], global_step)\n    writer.add_scalar('losses/value_loss', loss_value.item(), global_step)\n    writer.add_scalar('losses/policy_loss', loss_policy.item(), global_step)\n    writer.add_scalar('losses/entropy', loss_entropy.item(), global_step)\n    writer.add_scalar('losses/approx_kl', approx_kl.item(), global_step)\n    writer.add_scalar('losses/clipfrac', mean_clipfracs, global_step)\n    writer.add_scalar('losses/explained_variance', explained_var, global_step)\n    writer.add_scalar('charts/SPS', sps, global_step)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T14:31:44.596241Z","iopub.execute_input":"2023-09-09T14:31:44.596710Z","iopub.status.idle":"2023-09-09T14:31:44.601961Z","shell.execute_reply.started":"2023-09-09T14:31:44.596674Z","shell.execute_reply":"2023-09-09T14:31:44.600742Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"<img src=\"https://i.imgur.com/xC8UoKR.gif\" \n     align=\"right\" \n     width=\"300\" />\n### Final Thoughs \n     \nWe succeeding in implementing a state of the art PPO implementation for Atari environments. This project could easily be extended for use into other domains such as MuJoCo or custom environments. Doing so is something I intend to do in the near future.","metadata":{}}]}