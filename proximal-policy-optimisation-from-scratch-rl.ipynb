{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e23a1fe3",
   "metadata": {
    "papermill": {
     "duration": 0.008458,
     "end_time": "2023-08-19T08:05:46.829884",
     "exception": false,
     "start_time": "2023-08-19T08:05:46.821426",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# The Proximal Policy Optimization (PPO) Algorithm, from Scratch\n",
    "\n",
    "### Objective of this Notebook:\n",
    "The goal of this notebook is understand the Proximal Policy Optimization (PPO) algorithm. The objectives of this notebook are as follows:\n",
    "\n",
    "1. **Understanding of Proximal Policy Optimization**: Introducing PPO and its position in the RL landscape.\n",
    "2. **PPO algorithm explanation**: Break down the PPO algorithm, explaining its various components, including the clipping mechanism, advantage estimation, and policy updating.\n",
    "3. **Code Implementation in Modified Environment**: A step-by-step guide to implementing PPO in the modified Gym Pendulum environment. This includes adaptations like converting the continuous action space into a discrete one and scaling variables.\n",
    "\n",
    "## A Brief Overview of Proximal Policy Optimization\n",
    "In reinforcement learning, PPO has emerged as a powerful algorithm that balances the trade-off between efficiency and ease of implementation. Unlike some policy gradient methods, PPO aims to take larger policy update steps without incurring potential stability issues.\n",
    "\n",
    "The core idea of PPO is to optimize the policy in a way that doesn't change it too drastically from one iteration to the next. This \"proximal\" constraint helps to stabilize training, and it is typically achieved through a specialized clipping mechanism.\n",
    "\n",
    "# PPO Pseudocode\n",
    "\n",
    "1. Input: initial policy parameters $ \\theta_0 $, initial value function parameters $ \\phi_0 $\n",
    "2. **for** $ k = 0, 1, 2, ... $ **do**\n",
    "3. $\\quad$ Collect set of trajectories $ \\text{D}_k = \\{\\tau_i\\} $ by running policy $ \\pi_k = \\pi (\\theta_k) $ in the environment.\n",
    "4. $\\quad$ Compute rewards-to-go $ \\hat{R}_t $.\n",
    "5. $\\quad$ Compute advantage estimates, $\\hat{A}$ (using any method of advantage estimation) based on the current value function $ V_{\\phi_k} $.\n",
    "6. $\\quad$ Update the policy by maximizing the PPO-Clip objective:\n",
    "\n",
    "$$\n",
    "\\theta_{k+1} = \\arg \\max_\\theta \\dfrac{1}{|\\text{D}_k|T} \\sum_{\\tau \\in \\text{D}_k} \\sum_{t=0}^{T} \\min \\left(\\dfrac{\\pi_\\theta (a_t | s_t)}{\\pi_{\\theta_k} (a_t | s_t)} A ^ {\\pi_{\\theta_k}} (s_t, a_t), \\: \\text{clip}\\left(\\dfrac{\\pi_\\theta (a_t | s_t)}{\\pi_{\\theta_k} (a_t | s_t)}, 1 - \\epsilon, 1 + \\epsilon \\right) A ^ {\\pi_{\\theta_k}}(s_t, a_t) \\right)\n",
    "$$\n",
    "\n",
    "$\\quad$ typically via SGD with Adam.\n",
    "\n",
    "7. Fit value function by regression on MSE:\n",
    "\n",
    "$$\n",
    "\\phi_{k + 1} = \\arg \\min_\\phi \\dfrac{1}{|\\text{D}_k|T} \\sum_{\\tau \\in \\text{D}_k} \\sum_{t=0}^{T} \\left( V_\\phi (s_t) - \\hat{R}_t\\right)^2 ,\n",
    "$$\n",
    "\n",
    "$\\quad$ typically via some gradient descent algorithm.\n",
    "\n",
    "8. **end for**\n",
    "\n",
    "## Our Environment\n",
    "We're going to use the Gym Pendulum environment, with a few simple modifications:\n",
    "1. We're going to convert the continuous action space to a discrete one.\n",
    "2. We will scale the angular velocity variable so it's of comparable magnitude to the angles.\n",
    "3. We scale down the reward by a factor of 1/10, as this will likely be eaiser on our neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe313989",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-19T08:05:46.848157Z",
     "iopub.status.busy": "2023-08-19T08:05:46.847587Z",
     "iopub.status.idle": "2023-08-19T08:05:47.291140Z",
     "shell.execute_reply": "2023-08-19T08:05:47.289867Z"
    },
    "papermill": {
     "duration": 0.455934,
     "end_time": "2023-08-19T08:05:47.293959",
     "exception": false,
     "start_time": "2023-08-19T08:05:46.838025",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "class DiscreteActionWrapper(gym.ActionWrapper):\n",
    "    \"Bin continuous actions into discrete intervals.\"\n",
    "    def __init__(self, env, n_actions=5):\n",
    "        super().__init__(env)\n",
    "        self.n_actions = n_actions\n",
    "        self.action_space = gym.spaces.Discrete(n_actions)\n",
    "\n",
    "    def action(self, action):\n",
    "        if action == 0:\n",
    "            return np.array([-2])\n",
    "        elif action == 1:\n",
    "            return np.array([-1])\n",
    "        elif action == 2:\n",
    "            return np.array([0])\n",
    "        elif action == 3:\n",
    "            return np.array([1])\n",
    "        elif action == 4:\n",
    "            return np.array([2])\n",
    "    \n",
    "class ObservationWrapper(gym.ObservationWrapper):\n",
    "    \"Scale the third value of the observation by 1/8.\"\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation[2] *= 1/8.0\n",
    "        return observation\n",
    "    \n",
    "class RewardScalingWrapper(gym.RewardWrapper):\n",
    "    \"Scale the reward by the given factor.\"\n",
    "    def __init__(self, env, scaling_factor=1/10.0):\n",
    "        super().__init__(env)\n",
    "        self.scaling_factor = scaling_factor\n",
    "\n",
    "    def reward(self, reward):\n",
    "        return reward * self.scaling_factor\n",
    "    \n",
    "env = gym.make('Pendulum-v1')\n",
    "env = RewardScalingWrapper(DiscreteActionWrapper(ObservationWrapper(env)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95aa073d",
   "metadata": {
    "papermill": {
     "duration": 0.007946,
     "end_time": "2023-08-19T08:05:47.309826",
     "exception": false,
     "start_time": "2023-08-19T08:05:47.301880",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Step 1: Initialize Policy and Value Function Paramters \n",
    "\n",
    "In our approach, we deploy a conventional Actor-Critic architecture for both the policy (actor) and value function (critic).\n",
    "\n",
    "- **Policy** (Actor): The actor accepts a state tensor as input and produces logits. These logits correspond to the relative likelihoods of the actions to be chosen. Essentially, the policy dictates how our agent decides to act given the current state.\n",
    "\n",
    "- **Value Function** (Critic): The critic's role is to estimate the value of the given state. By accepting a state tensor, the critic provides a quantifiable measure of how \"good\" the state is, guiding the agent's decisions.\n",
    "\n",
    "- **Old Policy** for Probability Ratio Calculation: We also maintain an \"old\" policy that represents the policy from the previous time step. This old policy plays a vital role in calculating the probability ratio used in the PPO algorithm. Initially, the current policy and the old policy are identical, since there is no prior policy to reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a78f95ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-19T08:05:47.327726Z",
     "iopub.status.busy": "2023-08-19T08:05:47.326554Z",
     "iopub.status.idle": "2023-08-19T08:05:51.254896Z",
     "shell.execute_reply": "2023-08-19T08:05:51.253685Z"
    },
    "papermill": {
     "duration": 3.939999,
     "end_time": "2023-08-19T08:05:51.257522",
     "exception": false,
     "start_time": "2023-08-19T08:05:47.317523",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "hidden_size = 64\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, input_size, n_actions):\n",
    "        super().__init__()\n",
    "        self.dense_1 = nn.Linear(input_size, hidden_size)\n",
    "        self.dense_2 = nn.Linear(hidden_size, hidden_size//2)\n",
    "        self.dense_3 = nn.Linear(hidden_size//2, n_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.dense_1(x))\n",
    "        x = F.relu(self.dense_2(x))\n",
    "        x = self.dense_3(x)\n",
    "        return x\n",
    "    \n",
    "class ValueFunction(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.dense_1 = nn.Linear(input_size, hidden_size)\n",
    "        self.dense_2 = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.dense_1(x))\n",
    "        x = self.dense_2(x)\n",
    "        return x\n",
    "    \n",
    "n_actions = env.action_space.n\n",
    "input_size = env.observation_space.shape[0]\n",
    "\n",
    "policy = Policy(input_size, n_actions)\n",
    "value_function = ValueFunction(input_size)\n",
    "\n",
    "old_policy = Policy(input_size, n_actions)\n",
    "old_policy.load_state_dict(policy.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c93e8f",
   "metadata": {
    "papermill": {
     "duration": 0.007607,
     "end_time": "2023-08-19T08:05:51.273179",
     "exception": false,
     "start_time": "2023-08-19T08:05:51.265572",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Step 2: Main Training Loop\n",
    "In this step, we set up the core of our training process:\n",
    "\n",
    "- **Initialize and Reset Environment**: Start by resetting the environment and storing the initial observations.\n",
    "- **Define Training Parameters**: Set up essential training parameters like learning rate, epochs, batch size, etc.\n",
    "- **Initialize Optimizers**: Get the optimizers ready for updating the policy and value function during training.\n",
    "\n",
    "This phase ensures we have everything in place to run the training loop for PPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58a7577d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-19T08:05:51.290861Z",
     "iopub.status.busy": "2023-08-19T08:05:51.290204Z",
     "iopub.status.idle": "2023-08-19T08:05:51.299952Z",
     "shell.execute_reply": "2023-08-19T08:05:51.298747Z"
    },
    "papermill": {
     "duration": 0.021465,
     "end_time": "2023-08-19T08:05:51.302435",
     "exception": false,
     "start_time": "2023-08-19T08:05:51.280970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "training_steps = 10000\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "epsilon = 0.2\n",
    "\n",
    "learning_rate_policy = 0.0003\n",
    "learning_rate_value = 0.001\n",
    "\n",
    "optimizer_policy = torch.optim.Adam(policy.parameters(), lr=learning_rate_policy)\n",
    "optimizer_value_function = torch.optim.Adam(value_function.parameters(), lr=learning_rate_value)\n",
    "\n",
    "transitions = []\n",
    "\n",
    "state, info = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1729908b",
   "metadata": {
    "papermill": {
     "duration": 0.0074,
     "end_time": "2023-08-19T08:05:51.317747",
     "exception": false,
     "start_time": "2023-08-19T08:05:51.310347",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Step 3: Collect Trajectories\n",
    "\n",
    "In this step, we gather the trajectories that will guide the training of our PPO model:\n",
    "\n",
    "- **Environment Handling**: Though often done with multiple environments in parallel, we'll collect trajectories using only a single environment. The process remains consistent in both cases.\n",
    "- **Collecting Transition Tuples**: We'll gather tuples of `(state_0, action_0, reward, state_1, done)` until we reach a set number of transitions, typically controlled by a `batch_size` hyperparameter. For this code, we'll use a batch size of 64.\n",
    "- **State to Tensor Conversion**: Convert the current state to a Torch tensor and feed it through the policy network to obtain logits.\n",
    "- **Action Sampling**: Convert the logits to a probability distribution and sample an action, which is then sent to the environment to obtain the next state, reward, and done flag.\n",
    "- **Transition Storage**: Append the transition to a list. If the list reaches the batch size, move to the next step for learning. Otherwise, repeat this step to collect more transitions.\n",
    "\n",
    "By following this process, we build the foundational data needed for the PPO algorithm to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b928dcc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-19T08:05:51.335113Z",
     "iopub.status.busy": "2023-08-19T08:05:51.334602Z",
     "iopub.status.idle": "2023-08-19T08:05:51.495239Z",
     "shell.execute_reply": "2023-08-19T08:05:51.494225Z"
    },
    "papermill": {
     "duration": 0.172513,
     "end_time": "2023-08-19T08:05:51.497924",
     "exception": false,
     "start_time": "2023-08-19T08:05:51.325411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(training_steps):\n",
    "    # Convert state to a tensor\n",
    "    state_tensor = torch.tensor(state, dtype=torch.float, device=device)\n",
    "\n",
    "    # Pass state tensor through policy network\n",
    "    logits = policy(state_tensor)\n",
    "\n",
    "    # Convert logits to probability distribution\n",
    "    dist = torch.distributions.Categorical(logits=logits)\n",
    "\n",
    "    # Sample action from probability distribution\n",
    "    action = dist.sample().item()\n",
    "\n",
    "    # Execute action in environment\n",
    "    next_state, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "    transition = (state, action, reward, next_state, done)\n",
    "    transitions.append(transition)\n",
    "    \n",
    "    # Reset environment if done\n",
    "    if done:\n",
    "        state, info = env.reset()\n",
    "    else:\n",
    "        state = next_state\n",
    "\n",
    "    if len(transitions) < batch_size:\n",
    "        continue\n",
    "        \n",
    "    # In the actual code, there's no break here.\n",
    "    # We just need it because we can't write a for loop over several cells.\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c3c0b8",
   "metadata": {
    "papermill": {
     "duration": 0.008898,
     "end_time": "2023-08-19T08:05:51.514729",
     "exception": false,
     "start_time": "2023-08-19T08:05:51.505831",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Step 4: Compute Rewards-to-Go\n",
    "Computing the empirical return (rewards-to-go) is achieved through the following equation:\n",
    "\n",
    "$$\n",
    "\\hat{R}_t = r_t + \\gamma \\cdot r_{t+1} + \\gamma^2 \\cdot r_{t+2} + ... + \\gamma^n \\cdot V_\\phi(s_{t + n})\n",
    "$$\n",
    "\n",
    "Rewards-to-go provide an empirical estimate for $Q(s_t, a_t)$, which we us to compute the advantage later on.\n",
    "\n",
    "- **Utilizing Future Rewards**: We draw from rewards in future episodes to estimate the value of $ r_{t+i} $ and employ the value function to approximate the discounted reward until the end of the episode $ V_\\phi(s_{t + n}) $ when no more reward information is available.\n",
    "- **Elegance of Approach**: This method requires computing the value of only the final state in the trajectory, making it an elegant solution.\n",
    "- **Iterating Backwards**: The calculation is performed by iterating backwards over the rewards, simplifying the programming.\n",
    "- **Bootstrapping with Value Function**: Including a value function approximation for on-policy methods allows training on incomplete trajectories, increasing bias, reducing variance, and enhancing generalization.\n",
    "\n",
    "This step carefully integrates the observed rewards with value function approximations to build more reliable targets for our PPO training. It represents an essential balance between empirical information and model-based estimates, driving the policy towards better decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "356a6af5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-19T08:05:51.532059Z",
     "iopub.status.busy": "2023-08-19T08:05:51.531636Z",
     "iopub.status.idle": "2023-08-19T08:05:51.540856Z",
     "shell.execute_reply": "2023-08-19T08:05:51.539851Z"
    },
    "papermill": {
     "duration": 0.020814,
     "end_time": "2023-08-19T08:05:51.543251",
     "exception": false,
     "start_time": "2023-08-19T08:05:51.522437",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "states, actions, rewards, next_states, dones = zip(*transitions)\n",
    "\n",
    "# If the final step didn't cause the environment to terminate or timeout\n",
    "if not dones[-1]:\n",
    "    # Calculate the value of the final state\n",
    "    final_state_tensor = torch.tensor(next_states[-1], dtype=torch.float, device=device)\n",
    "    final_state_value = value_function(final_state_tensor).item()\n",
    "else:\n",
    "    # If it did finish, there is no future discounted reward, so it's just 0\n",
    "    final_state_value = 0\n",
    "\n",
    "# Initialize rewards to go list\n",
    "rewards_to_go = []\n",
    "\n",
    "# R represents all the future rewards\n",
    "# In the above equation, anything being multiplied by gamma is a part of R\n",
    "R = final_state_value\n",
    "\n",
    "# Iterate backwards\n",
    "for r, done in zip(reversed(rewards), reversed(dones)):\n",
    "    # If the environment is done, there is no future reward\n",
    "    if done:\n",
    "        R = 0\n",
    "    # Add the immediate reward and discount the future rewards\n",
    "    R = r + gamma * R\n",
    "\n",
    "    # Add the calculated reward-to-go to the list at the start as we're iterating backwards\n",
    "    rewards_to_go.insert(0, R)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b5733c",
   "metadata": {
    "papermill": {
     "duration": 0.008194,
     "end_time": "2023-08-19T08:05:51.559366",
     "exception": false,
     "start_time": "2023-08-19T08:05:51.551172",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Step 5: Compute Advantage Estimates\n",
    "In RL, the concept of advantage serves as a measure of how much better a particular action is compared to the average action in a given state. \n",
    "\n",
    "The mathematical formulation of advantage, $ A(s,a) $, is expressed as the difference between the Q-value ofo a state-action pair, $ Q(s,a) $ and the value of a state, $ V(s) $.\n",
    "\n",
    "$$\n",
    "A(s, a) = Q(s, a) - V(s)\n",
    "$$\n",
    "\n",
    "To calculate the advantage, we need to break down the components involved:\n",
    "- **Q-value**: The Q-value represents the expected return of taking action $a$ in state $s$ and following the current policy thereafter. Using the Bellman equation, it can be expressed as:\n",
    "\n",
    "$$\n",
    "Q(s,a) = r + \\gamma V(s')\n",
    "$$\n",
    "\n",
    "Here, $r$ is the immediate reward, and $\\gamma \\cdot V(s)$ is the expected future discounted reward from the next state $s'$.\n",
    "\n",
    "- **Rewards-to-go**: To get a more accurate estimate of $Q(s,a)$, we can unravel the Bellman equation several steps into the future:\n",
    "\n",
    "$$\n",
    "Q(s_0,a_0) = r_0 + \\gamma r_1 + \\gamma^2 r_2 + ... + \\gamma^n V(s_n)\n",
    "$$\n",
    "\n",
    "This series sums the rewards at each step, discounted by $\\gamma$, plus the value of the final state in the trajectory, discounted appropriately. This sum is the rewards-to-go.\n",
    "\n",
    "- **Advantage Calculation**: Finally, by combining the Q-value with the state value, we arrive at our expression for the advantage:\n",
    "\n",
    "$$\n",
    "A(s,a) = \\text{reward-to-go} - V_\\phi(s)\n",
    "$$\n",
    "\n",
    "Here, $V_\\phi(s)$ is the value function's approximation of the state value, parameterized by $\\phi$. The value function is trained to predict this quantity, and it is reused here to approximate V(s) for all states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a12f3e8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-19T08:05:51.577618Z",
     "iopub.status.busy": "2023-08-19T08:05:51.576643Z",
     "iopub.status.idle": "2023-08-19T08:05:51.590772Z",
     "shell.execute_reply": "2023-08-19T08:05:51.589659Z"
    },
    "papermill": {
     "duration": 0.026011,
     "end_time": "2023-08-19T08:05:51.593671",
     "exception": false,
     "start_time": "2023-08-19T08:05:51.567660",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert states to a tensor\n",
    "states_tensor = torch.tensor(np.array(states), dtype=torch.float, device=device)\n",
    "\n",
    "# Convert rewards-to-go to a tensor\n",
    "rewards_tensor = torch.tensor(np.array(rewards_to_go), dtype=torch.float, device=device)\n",
    "\n",
    "# Get state values with value function\n",
    "values = value_function(states_tensor).squeeze(-1)\n",
    "\n",
    "# Calculate advantages\n",
    "advantages = rewards_tensor - values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72262ea1",
   "metadata": {
    "papermill": {
     "duration": 0.007633,
     "end_time": "2023-08-19T08:05:51.609331",
     "exception": false,
     "start_time": "2023-08-19T08:05:51.601698",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Step 6: Update the policy by maximizing the PPO-Clip objective:\n",
    "\n",
    "$$\n",
    "\\theta_{k+1} = \\arg \\max_\\theta \\dfrac{1}{|\\text{D}_k|T} \\sum_{\\tau \\in \\text{D}_k} \\sum_{t=0}^{T} \\min \\left(\\dfrac{\\pi_\\theta (a_t | s_t)}{\\pi_{\\theta_k} (a_t | s_t)} A ^ {\\pi_{\\theta_k}} (s_t, a_t), \\; \\text{clip}\\left(\\dfrac{\\pi_\\theta (a_t | s_t)}{\\pi_{\\theta_k} (a_t | s_t)}, 1 - \\epsilon, 1 + \\epsilon \\right) A ^ {\\pi_{\\theta_k}}(s_t, a_t) \\right)\n",
    "$$\n",
    "\n",
    "$\\quad$ typically via SGD with Adam.\n",
    "\n",
    "Here's a breakdown of each component of the formula:\n",
    "\n",
    "- **The policy ratio**, $ r_t(\\theta) $: This is the ratio of the probability of taking action $a_t$ under new policy parameters $\\theta$ to the old policy parameters $\\theta_k$. \n",
    "\n",
    "    We keep track of the old policy so we can calculate this value. Values greater than 1 indicates the action is being selected more frequently, values lower than 1 indicate the action being selected less frequently.\n",
    "\n",
    "    It is defined as:\n",
    "\n",
    "$$\n",
    "\\dfrac{\\pi_\\theta (a_t | s_t)}{\\pi_{\\theta_k} (a_t | s_t)}\n",
    "$$\n",
    "\n",
    "\n",
    "- **Clipped policy ratio**: To ensure stability, the policy ratio is clipped within the range $[1 - \\epsilon, \\; 1+\\epsilon]$. This prevents the policy from changing too dramatically in a single step.\n",
    "\n",
    "$$\n",
    "\\text{clip}\\left(r_t(\\theta), \\; 1 - \\epsilon, 1 + \\epsilon \\right)\n",
    "$$\n",
    "\n",
    "- **Advantage function** $ A ^ {\\pi_{\\theta_k}} (s_t, a_t) $: This represents how much better taking a specific action is compared to the average over all possible actions in the given state, according to the old policy.\n",
    "\n",
    "- **Objective function**: The whole expression encapsulates the PPO-clip objective that we want to maximize. It is designed to favor actions that have a positive advantage, while constraining the policy from changing too drastically.\n",
    "\n",
    "$$ \n",
    "\\min \\left(r_t(\\theta) A ^ {\\pi_{\\theta_k}} (s_t, a_t), \\; \\text{clip}\\left(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon \\right) A ^ {\\pi_{\\theta_k}}(s_t, a_t) \\right)\n",
    "$$\n",
    "\n",
    "Let's break down this objective into two possible scenarios: when the advantage is positive and when it is negative.\n",
    "\n",
    "- Positive Advantage: When the advantage is positive, it means taking the action is better than the average, and we want to increase the probability of selecting it. \n",
    "\n",
    "Suppose our policy ratio $ r_t(\\theta) = 1.2 $ (indicating the action is favored more by the new policy) and the advantage $ A ^ {\\pi_{\\theta_k}} (s_t, a_t) = 0.5 $, with $ \\epsilon = 0.1 $:\n",
    "\n",
    "- LHS (Left-Hand Side): $ 1.2 \\times 0.5 = 0.6 $\n",
    "- RHS (Right-Hand Side): $ \\text{clip}(1.2, 1 - 0.1, 1 + 0.1) \\times 0.5 = 1.1 \\times 0.5 = 0.55 $\n",
    "\n",
    "In this case, the minimum value is taken, so the final result would be $ 0.55 $.\n",
    "\n",
    "- Negative Advantage: When the advantage is negative, it means the action is worse than the average, and we want to decrease the probability of selecting it.\n",
    "\n",
    "Suppose our policy ratio $ r_t(\\theta) = 0.8 $ (indicating the action is less favored by the new policy) and the advantage $ A ^ {\\pi_{\\theta_k}} (s_t, a_t) = -0.5 $, with $ \\epsilon = 0.1 $:\n",
    "\n",
    "- LHS: $ 0.8 \\times (-0.5) = -0.4 $\n",
    "- RHS: $ \\text{clip}(0.8, 1 - 0.1, 1 + 0.1) \\times (-0.5) = 0.9 \\times (-0.5) = -0.45 $\n",
    "\n",
    "Again, the minimum value is taken, so the final result would be $ -0.45 $.\n",
    "\n",
    "These two examples demonstrate how the objective function in PPO simultaneously attempts to increase the probability of actions that are better than the average and decrease the probability of actions that are worse than the average. The clipping mechanism ensures that these updates are controlled, promoting stability in the training process.\n",
    "\n",
    "- **Averaging Over Trajectories and Time Steps**: The outer summation and division by $|\\text{D}_k|$ average the objective over all the trajectories in the dataset $\\text{D}_k$ and all the time steps up to {T}. This makes the objective representative of the policy's performance over the entire dataset of trajectories. In practice\n",
    "\n",
    "- **Policy update**: The final step is to actually update the policy parameters $\\theta$ to $\\theta_{k+1}$ by maximizing the objective. This is done using stochastic gradient descent (typically Adam) to find the values of $\\theta$ that maximize this objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb59dfef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-19T08:05:51.626662Z",
     "iopub.status.busy": "2023-08-19T08:05:51.626219Z",
     "iopub.status.idle": "2023-08-19T08:05:51.688472Z",
     "shell.execute_reply": "2023-08-19T08:05:51.687507Z"
    },
    "papermill": {
     "duration": 0.07405,
     "end_time": "2023-08-19T08:05:51.691181",
     "exception": false,
     "start_time": "2023-08-19T08:05:51.617131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First, calculate policy ratio\n",
    "# Convert actions to tensor and unsqueeze for upcoming gather\n",
    "actions_tensor = torch.tensor(actions, dtype=torch.long, device=device).unsqueeze(-1)\n",
    "\n",
    "# For current policy\n",
    "logits_current = policy(states_tensor)\n",
    "probs_current = F.softmax(logits_current, dim=-1)\n",
    "action_probs_current = probs_current.gather(dim=1, index=actions_tensor)\n",
    "\n",
    "# For old policy (use no_grad() as we never update the old policy with backprop)\n",
    "with torch.no_grad():\n",
    "    logits_old = old_policy(states_tensor)\n",
    "    probs_old = F.softmax(logits_old, dim=-1)\n",
    "    action_probs_old = probs_old.gather(dim=1, index=actions_tensor)\n",
    "\n",
    "# Calculate the policy ratio\n",
    "policy_ratio = (action_probs_current/action_probs_old).squeeze(-1)\n",
    "\n",
    "# Next, calculate clipped policy ratio\n",
    "# This is a very simple calculation as we've already calculated the policy ratio\n",
    "clipped_policy_ratio = torch.clip(policy_ratio, min=1-epsilon, max=1+epsilon)\n",
    "\n",
    "# Next, we calculate the objective function\n",
    "objective = torch.min(policy_ratio * advantages, clipped_policy_ratio * advantages)\n",
    "\n",
    "# We then average this across all transitions and episodes using mean to get the policy loss\n",
    "# We use negative as we want to maximize it and loss metrics are to be minimized\n",
    "loss_policy = -torch.mean(objective)\n",
    "\n",
    "# First, set our old policy to our current policy before we update it\n",
    "old_policy.load_state_dict(policy.state_dict())\n",
    "\n",
    "# Backward pass for policy\n",
    "optimizer_policy.zero_grad()\n",
    "loss_policy.backward()\n",
    "optimizer_policy.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3213e8",
   "metadata": {
    "papermill": {
     "duration": 0.008228,
     "end_time": "2023-08-19T08:05:51.707409",
     "exception": false,
     "start_time": "2023-08-19T08:05:51.699181",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Step 7: Fit value function by regression on MSE:\n",
    "\n",
    "$$\n",
    "\\phi_{k + 1} = \\arg \\min_\\phi \\dfrac{1}{|\\text{D}_k|T} \\sum_{\\tau \\in \\text{D}_k} \\sum_{t=0}^{T} \\left( V_\\phi (s_t) - \\hat{R}_t\\right)^2 ,\n",
    "$$\n",
    "\n",
    "$\\quad$ typically via some gradient descent algorithm.\n",
    "\n",
    "The goal of this process is to make the value function approximate the expected returns under the current policy. In other words, we want to find a function $V_\\phi(s)$ that best predicts what the actual return will be if we follow our current policy from state $s$.\n",
    "\n",
    "The equation above says that we're looking to minimize the mean squared error (MSE) between the predicted state value using our value function $V_\\phi(s_t)$ and the empirically observed rewards (previously calculated rewards-to-go).\n",
    "\n",
    "We previously used our rewards-to-go to esimate the Q-value $Q(s,a)$ in our advantage calculation $A(s,a) = Q(s,a) - V(s)$, so why are we now using it to describe a target for the value function?\n",
    "\n",
    "The rewards-to-go are an empirical estimate of the expected return if action $a$ is taken in state $s$, and the current policy is followed thereafter. They essentially serve as an estimate for $Q(s,a)$ for the current policy. \n",
    "\n",
    "This metric hence serves to guide our value function towards more accurately approximating the value of a state under our current policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05f3ca5",
   "metadata": {
    "papermill": {
     "duration": 0.007582,
     "end_time": "2023-08-19T08:05:51.722876",
     "exception": false,
     "start_time": "2023-08-19T08:05:51.715294",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Step 7: Fit Value Function by Regression on MSE\n",
    "\n",
    "We optimize the value function parameters according to the equation:\n",
    "\n",
    "$$\n",
    "\\phi_{k + 1} = \\arg \\min_\\phi \\dfrac{1}{|\\text{D}_k|T} \\sum_{\\tau \\in \\text{D}_k} \\sum_{t=0}^{T} \\left( V_\\phi (s_t) - \\hat{R}_t\\right)^2 ,\n",
    "$$\n",
    "\n",
    "This is typically achieved through a gradient descent algorithm like Adam.\n",
    "\n",
    "- **Objective**: The goal is to find the value function $V_\\phi(s)$ that best approximates the expected returns under the current policy starting from state $s$.\n",
    "- **Minimizing MSE**: We want to minimize the mean squared error (MSE) between the predicted value function $V_\\phi(s_t)$ and the empirically observed rewards (rewards-to-go).\n",
    "- **Utilizing Rewards-to-Go**: Previously used to estimate the Q-value $Q(s,a)$ in our advantage calculation $A(s,a) = Q(s,a) - V(s)$, rewards-to-go now guide our value function to more accurately predict the value of a state under our policy.\n",
    "\n",
    "By linking the value function to empirical returns, this step ensures that the value function stays in line with the policy's actual performance in the environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9295919",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-19T08:05:51.742168Z",
     "iopub.status.busy": "2023-08-19T08:05:51.741340Z",
     "iopub.status.idle": "2023-08-19T08:05:51.754137Z",
     "shell.execute_reply": "2023-08-19T08:05:51.752911Z"
    },
    "papermill": {
     "duration": 0.026105,
     "end_time": "2023-08-19T08:05:51.756834",
     "exception": false,
     "start_time": "2023-08-19T08:05:51.730729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predict values for current state\n",
    "values = value_function(states_tensor).squeeze(-1)\n",
    "\n",
    "loss_value_function = F.mse_loss(values, rewards_tensor)\n",
    "\n",
    "# Backward pass for value function\n",
    "optimizer_value_function.zero_grad()\n",
    "loss_value_function.backward()\n",
    "optimizer_value_function.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d61182",
   "metadata": {
    "papermill": {
     "duration": 0.007556,
     "end_time": "2023-08-19T08:05:51.772570",
     "exception": false,
     "start_time": "2023-08-19T08:05:51.765014",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Done\n",
    "And that's it! We've gone through one full pass of the PPO algorithm, detailing each step and understanding the mechanics that make this algorithm effective.\n",
    "\n",
    "## Full Code\n",
    "Now, we will bring all these pieces together to create a complete code implementation for PPO in our modified Gym Pendulum environment. Below is the full code that combines all the steps, including environment setup, hyperparameter initialization, training loop, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a1872da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-19T08:05:51.790516Z",
     "iopub.status.busy": "2023-08-19T08:05:51.790083Z",
     "iopub.status.idle": "2023-08-19T08:26:09.148286Z",
     "shell.execute_reply": "2023-08-19T08:26:09.146814Z"
    },
    "papermill": {
     "duration": 1217.370998,
     "end_time": "2023-08-19T08:26:09.151445",
     "exception": false,
     "start_time": "2023-08-19T08:05:51.780447",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Environment\n",
    "class DiscreteActionWrapper(gym.ActionWrapper):\n",
    "    \"Bin continuous actions into discrete intervals.\"\n",
    "    def __init__(self, env, n_actions=5):\n",
    "        super().__init__(env)\n",
    "        self.n_actions = n_actions\n",
    "        self.action_space = gym.spaces.Discrete(n_actions)\n",
    "\n",
    "    def action(self, action):\n",
    "        if action == 0:\n",
    "            return np.array([-2])\n",
    "        elif action == 1:\n",
    "            return np.array([-1])\n",
    "        elif action == 2:\n",
    "            return np.array([0])\n",
    "        elif action == 3:\n",
    "            return np.array([1])\n",
    "        elif action == 4:\n",
    "            return np.array([2])\n",
    "    \n",
    "class ObservationWrapper(gym.ObservationWrapper):\n",
    "    \"Scale the third value of the observation by 1/8.\"\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation[2] *= 1/8.0\n",
    "        return observation\n",
    "    \n",
    "class RewardScalingWrapper(gym.RewardWrapper):\n",
    "    \"Scale the reward by the given factor.\"\n",
    "    def __init__(self, env, scaling_factor=1/10.0):\n",
    "        super().__init__(env)\n",
    "        self.scaling_factor = scaling_factor\n",
    "\n",
    "    def reward(self, reward):\n",
    "        return reward * self.scaling_factor\n",
    "    \n",
    "# Networks\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, input_size, n_actions):\n",
    "        super().__init__()\n",
    "        self.dense_1 = nn.Linear(input_size, hidden_size)\n",
    "        self.dense_2 = nn.Linear(hidden_size, hidden_size//2)\n",
    "        self.dense_3 = nn.Linear(hidden_size//2, n_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.dense_1(x))\n",
    "        x = F.relu(self.dense_2(x))\n",
    "        x = self.dense_3(x)\n",
    "        return x\n",
    "    \n",
    "class ValueFunction(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.dense_1 = nn.Linear(input_size, hidden_size)\n",
    "        self.dense_2 = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.dense_1(x))\n",
    "        x = self.dense_2(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "# Make an environment\n",
    "env = gym.make('Pendulum-v1')\n",
    "env = RewardScalingWrapper(DiscreteActionWrapper(ObservationWrapper(env)))\n",
    "\n",
    "# 1. Initialize networks and old policy\n",
    "hidden_size = 64\n",
    "n_actions = env.action_space.n\n",
    "input_size = env.observation_space.shape[0]\n",
    "\n",
    "policy = Policy(input_size, n_actions)\n",
    "value_function = ValueFunction(input_size)\n",
    "\n",
    "old_policy = Policy(input_size, n_actions)\n",
    "old_policy.load_state_dict(policy.state_dict())\n",
    "\n",
    "# Define hyperparameters\n",
    "device = 'cpu'\n",
    "training_steps = 2000000\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "epsilon = 0.2\n",
    "\n",
    "learning_rate_policy = 0.0003\n",
    "learning_rate_value = 0.001\n",
    "\n",
    "optimizer_policy = torch.optim.Adam(policy.parameters(), lr=learning_rate_policy)\n",
    "optimizer_value_function = torch.optim.Adam(value_function.parameters(), lr=learning_rate_value)\n",
    "\n",
    "transitions = []\n",
    "\n",
    "# Reset environment\n",
    "state, info = env.reset()\n",
    "\n",
    "# 2. Main training loop\n",
    "for i in range(training_steps):\n",
    "    # 3. Gather trajectories\n",
    "    # Convert state to a tensor\n",
    "    state_tensor = torch.tensor(state, dtype=torch.float, device=device)\n",
    "\n",
    "    # Pass state tensor through policy network\n",
    "    logits = policy(state_tensor)\n",
    "\n",
    "    # Convert logits to probability distribution\n",
    "    dist = torch.distributions.Categorical(logits=logits)\n",
    "\n",
    "    # Sample action from probability distribution\n",
    "    action = dist.sample().item()\n",
    "\n",
    "    # Execute action in environment\n",
    "    next_state, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "    transition = (state, action, reward, next_state, done)\n",
    "    transitions.append(transition)\n",
    "    \n",
    "    # Reset environment if done\n",
    "    if done:\n",
    "        state, info = env.reset()\n",
    "    else:\n",
    "        state = next_state\n",
    "\n",
    "    if len(transitions) < batch_size:\n",
    "        continue\n",
    "        \n",
    "    # 4. Compute rewards to go\n",
    "    # Expand transitions\n",
    "    states, actions, rewards, next_states, dones = zip(*transitions)\n",
    "    \n",
    "    # Clear transitions\n",
    "    transitions = []\n",
    "\n",
    "    # If the final step didn't cause the environment to terminate or timeout\n",
    "    if not dones[-1]:\n",
    "        # Calculate the value of the final state\n",
    "        final_state_tensor = torch.tensor(next_states[-1], dtype=torch.float, device=device)\n",
    "        final_state_value = value_function(final_state_tensor).item()\n",
    "    else:\n",
    "        # If it did finish, there is no future discounted reward, so it's just 0\n",
    "        final_state_value = 0\n",
    "\n",
    "    # Initialize rewards to go list\n",
    "    rewards_to_go = []\n",
    "\n",
    "    # R represents all the future rewards\n",
    "    # In the above equation, anything being multiplied by gamma is a part of R\n",
    "    R = final_state_value\n",
    "\n",
    "    # Iterate backwards\n",
    "    for r, done in zip(reversed(rewards), reversed(dones)):\n",
    "        # If the environment is done, there is no future reward\n",
    "        if done:\n",
    "            R = 0\n",
    "        # Add the immediate reward and discount the future rewards\n",
    "        R = r + gamma * R\n",
    "\n",
    "        # Add the calculated reward-to-go to the list at the start as we're iterating backwards\n",
    "        rewards_to_go.insert(0, R)\n",
    "        \n",
    "    # 5. Advantage estimate\n",
    "    # Convert states to a tensor\n",
    "    states_tensor = torch.tensor(np.array(states), dtype=torch.float, device=device)\n",
    "\n",
    "    # Convert rewards-to-go to a tensor\n",
    "    rewards_tensor = torch.tensor(np.array(rewards_to_go), dtype=torch.float, device=device)\n",
    "\n",
    "    # Get state values with value function\n",
    "    values = value_function(states_tensor).squeeze(-1)\n",
    "\n",
    "    # Calculate advantages\n",
    "    advantages = rewards_tensor - values\n",
    "    \n",
    "    # 6. Calculate policy ratio loss and backward pass\n",
    "    # Convert actions to tensor and unsqueeze for upcoming gather\n",
    "    actions_tensor = torch.tensor(actions, dtype=torch.long, device=device).unsqueeze(-1)\n",
    "\n",
    "    # For current policy\n",
    "    logits_current = policy(states_tensor)\n",
    "    probs_current = F.softmax(logits_current, dim=-1)\n",
    "    action_probs_current = probs_current.gather(dim=1, index=actions_tensor)\n",
    "\n",
    "    # For old policy (use no_grad() as we never update the old policy with backprop)\n",
    "    with torch.no_grad():\n",
    "        logits_old = old_policy(states_tensor)\n",
    "        probs_old = F.softmax(logits_old, dim=-1)\n",
    "        action_probs_old = probs_old.gather(dim=1, index=actions_tensor)\n",
    "\n",
    "    # Calculate the policy ratio\n",
    "    policy_ratio = (action_probs_current/action_probs_old).squeeze(-1)\n",
    "\n",
    "    # Next, calculate clipped policy ratio\n",
    "    # This is a very simple calculation as we've already calculated the policy ratio\n",
    "    clipped_policy_ratio = torch.clip(policy_ratio, min=1-epsilon, max=1+epsilon)\n",
    "\n",
    "    # Next, we calculate the objective function\n",
    "    objective = torch.min(policy_ratio * advantages, clipped_policy_ratio * advantages)\n",
    "\n",
    "    # We then average this across all transitions and episodes using mean to get the policy loss\n",
    "    # We use negative as we want to maximize it and loss metrics are to be minimized\n",
    "    loss_policy = -torch.mean(objective)\n",
    "\n",
    "    # First, set our old policy to our current policy before we update it\n",
    "    old_policy.load_state_dict(policy.state_dict())\n",
    "\n",
    "    # Backward pass for policy\n",
    "    optimizer_policy.zero_grad()\n",
    "    loss_policy.backward()\n",
    "    optimizer_policy.step()\n",
    "    \n",
    "    # 7. Calculate value function loss and backward pass\n",
    "    # Predict values for current state\n",
    "    values = value_function(states_tensor).squeeze(-1)\n",
    "\n",
    "    loss_value_function = F.mse_loss(values, rewards_tensor)\n",
    "\n",
    "    # Backward pass for value function\n",
    "    optimizer_value_function.zero_grad()\n",
    "    loss_value_function.backward()\n",
    "    optimizer_value_function.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af99222a",
   "metadata": {
    "papermill": {
     "duration": 0.007604,
     "end_time": "2023-08-19T08:26:09.167493",
     "exception": false,
     "start_time": "2023-08-19T08:26:09.159889",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Visualization\n",
    "Here's how we can use our learned policy to execute actions in the environment, with an accompanying recording:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "994c75ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-19T08:26:09.185237Z",
     "iopub.status.busy": "2023-08-19T08:26:09.184821Z",
     "iopub.status.idle": "2023-08-19T08:26:09.308731Z",
     "shell.execute_reply": "2023-08-19T08:26:09.307272Z"
    },
    "papermill": {
     "duration": 0.136084,
     "end_time": "2023-08-19T08:26:09.311564",
     "exception": false,
     "start_time": "2023-08-19T08:26:09.175480",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create env\n",
    "env = gym.make('Pendulum-v1')\n",
    "env = RewardScalingWrapper(DiscreteActionWrapper(ObservationWrapper(env)))\n",
    "\n",
    "# Reset env\n",
    "state, info = env.reset()\n",
    "\n",
    "total_reward = 0\n",
    "\n",
    "while True:\n",
    "    state_tensor = torch.tensor(state, dtype=torch.float, device=device)\n",
    "    logits = policy(state_tensor)\n",
    "    dist = torch.distributions.Categorical(logits=logits)\n",
    "    action = dist.sample().item()\n",
    "\n",
    "    state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101b2c48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T16:15:14.176764Z",
     "iopub.status.busy": "2023-08-18T16:15:14.175944Z"
    },
    "papermill": {
     "duration": 0.00769,
     "end_time": "2023-08-19T08:26:09.327035",
     "exception": false,
     "start_time": "2023-08-19T08:26:09.319345",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "![](https://i.imgur.com/ykouf68.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6112ab",
   "metadata": {
    "papermill": {
     "duration": 0.007346,
     "end_time": "2023-08-19T08:26:09.342229",
     "exception": false,
     "start_time": "2023-08-19T08:26:09.334883",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Wrapping Up\n",
    "I hope you found this notebook useful. I intend to make notebooks in the future that incorporate modern tweaks into PPO to improve performance.\n",
    "\n",
    "Do check out my other work if you enjoyed this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1235.619296,
   "end_time": "2023-08-19T08:26:10.677650",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-08-19T08:05:35.058354",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
