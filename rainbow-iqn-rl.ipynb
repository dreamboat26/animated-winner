{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Rainbow IQN\nThe purpose of this notebook is to build an implementation of [DeepMind's Rainbow DQN](https://arxiv.org/abs/1710.02298) algorithm, substituting out the dated C51 tweak with the more modern IQN distributional tweak. \n\nRainbow is a DQN implementation with the following 6 modifications:\n\n- **[Double DQN](https://arxiv.org/abs/1509.06461)**: Addresses the overestimation of Q-values by decoupling selection and evaluation of the action.\n- **[Dueling DQN](https://arxiv.org/abs/1511.06581)**: Separates the representation of state values and the advantages of each action, enabling the learning of state values without needing the effect of each action.\n- **[N-step Learning]()**:  Uses a multi-step approach for updating Q-values, which helps in faster propagation of reward information.\n- **[Prioritised Experience Replay (PER)](https://arxiv.org/abs/1511.05952)**: Improves sample efficiency by replaying more important transitions more frequently.\n- **[Noisy Nets for Exploration](https://arxiv.org/abs/1706.10295)**: Integrates parameterised noise into network weights to facilitate exploration, as an alternative to epsilon-greedy exploration.\n- **[Categorical DQN](https://arxiv.org/abs/1707.06887)**: Instead of learning to model the state-action value function, Categorical DQN (C51) models the return distribution over a reward range using discretised bins. \n\nWe substitute C51 for the powerful [Implicit Quantile Networks](https://arxiv.org/abs/1806.06923) distributional model. IQN is similar to C51 in that it models the distribution of rewards, but instead of learning the Q-function as a set of discrete bins, IQN learns a full quantile function, modelling the full range of rewards for each quantile. For more information on IQN I have a tutorial on it here: [IQN Tutorial](https://www.kaggle.com/code/auxeno/implicit-quantile-networks-iqn-rl). I also have tutorials on the other 5 Rainbow modifications which can be found on my Kaggle page.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nimport gymnasium as gym\nfrom collections import deque\nimport random\nimport time\n\n### Noisy Linear Layer ###\nclass NoisyLinear(nn.Module):\n    \"Noisy networks linear layer.\"\n    def __init__(self, in_features, out_features, noise_std=0.4):\n        super().__init__()\n        \n        self.in_features = in_features\n        self.out_features = out_features\n        self.std_init = noise_std\n        \n        # Learnable parameters for mean and standard deviation of weights and biases\n        self.weight_mean = nn.Parameter(torch.Tensor(out_features, in_features))\n        self.weight_std  = nn.Parameter(torch.Tensor(out_features, in_features))\n        self.bias_mean   = nn.Parameter(torch.Tensor(out_features))\n        self.bias_std    = nn.Parameter(torch.Tensor(out_features))\n\n        # Buffers for noise\n        self.register_buffer('weight_noise', torch.Tensor(out_features, in_features))\n        self.register_buffer('bias_noise', torch.Tensor(out_features))\n        \n        self.initialize_parameters()\n        self.reset_noise()\n\n    def forward(self, x):\n        weights = self.weight_mean + self.weight_std * self.weight_noise if self.training else self.weight_mean\n        biases  = self.bias_mean   + self.bias_std   * self.bias_noise   if self.training else self.bias_mean\n        return torch.nn.functional.linear(x, weights, biases)\n    \n    def initialize_parameters(self):\n        # Initialise parameters using uniform distribution and std_init for scaling\n        initialization_range = 1. / np.sqrt(self.in_features)\n        \n        nn.init.uniform_(self.weight_mean, -initialization_range, initialization_range)\n        nn.init.uniform_(self.bias_mean, -initialization_range, initialization_range)\n        nn.init.constant_(self.weight_std, self.std_init / np.sqrt(self.in_features))\n        nn.init.constant_(self.bias_std, self.std_init / np.sqrt(self.out_features))\n    \n    def reset_noise(self):\n        # Regenerates noise for weights and biases\n        input_noise  = self.generate_noise(self.in_features)\n        output_noise = self.generate_noise(self.out_features)\n        \n        self.weight_noise.copy_(output_noise.outer(input_noise))\n        self.bias_noise.copy_(self.generate_noise(self.out_features))\n    \n    @staticmethod\n    def generate_noise(size):\n        # Generates scaled noise, transformed with sign of noise * square root of its abs value\n        noise = torch.randn(size)\n        return noise.sign() * torch.sqrt(torch.abs(noise))\n    \n\n### IQN Q-Network ###\nclass IQNetwork(nn.Module):\n    def __init__(self, state_dim, action_dim, hidden_dim=64, num_eval_quantiles=16, cosine_embedding_dim=16, noisy=False):\n        super().__init__()\n        self.num_actions = action_dim\n        self.hidden_dim = hidden_dim\n        self.num_eval_quantiles = num_eval_quantiles     \n        self.cosine_embedding_dim = cosine_embedding_dim\n        \n        self.input_layer  = nn.Linear(state_dim, hidden_dim)\n        self.hidden_layer = nn.Linear(hidden_dim, hidden_dim)\n        self.tau_embedding_layer = nn.Linear(cosine_embedding_dim, hidden_dim)\n        \n        if noisy:\n            self.value_layer = NoisyLinear(hidden_dim, 1)\n            self.advantage_layer = NoisyLinear(hidden_dim, self.num_actions)\n        else:\n            self.value_layer = nn.Linear(hidden_dim, 1)\n            self.advantage_layer = nn.Linear(hidden_dim, self.num_actions)\n        \n    def forward(self, state, taus=None):\n        if taus is None:\n            taus = self.generate_taus(batch_size=state.shape[0], uniform=True).to(state.device)\n        \n        assert state.shape[0] == taus.shape[0], \"Use same batch sizes for state and tau tensors.\"\n        batch_size, n_quantiles = taus.shape\n        \n        # State encoding\n        state_enc = F.relu(self.input_layer(state))\n        state_enc = F.relu(self.hidden_layer(state_enc))\n        \n        # Tau encoding\n        pi_i = torch.pi * torch.arange(self.cosine_embedding_dim, device=state.device)\n        cos_pi_i_tau = torch.cos(taus.unsqueeze(-1) * pi_i)\n        tau_enc = F.relu(self.tau_embedding_layer(cos_pi_i_tau.view(batch_size * n_quantiles, self.cosine_embedding_dim)))\n        \n        # Combine encodings with Hadamard product\n        combined_enc = state_enc.unsqueeze(-1) * tau_enc.view(batch_size, self.hidden_dim, n_quantiles)\n        \n        # Dueling\n        value = self.value_layer(combined_enc.view(batch_size * n_quantiles, self.hidden_dim))\n        advantages = self.advantage_layer(combined_enc.view(batch_size * n_quantiles, self.hidden_dim))\n        q_values = value + advantages - advantages.mean(dim=1, keepdim=True)\n        \n        return q_values.view(batch_size, self.num_actions, n_quantiles)\n    \n    def generate_taus(self, batch_size=1, uniform=False):\n        if uniform:\n            return torch.linspace(0, 1, self.num_eval_quantiles + 2)[1:-1].expand((batch_size, self.num_eval_quantiles))\n        return torch.rand((batch_size, self.num_eval_quantiles))\n    \n    def reset_noise(self):\n        self.value_layer.reset_noise()\n        self.advantage_layer.reset_noise()\n\n        \n### Segment Trees ####\nimport operator\n\nclass SegmentTree(object):\n    def __init__(self, capacity, operation, neutral_element):\n        assert capacity > 0 and capacity & (capacity - 1) == 0, \"Capacity must be a power of 2.\"\n        self._capacity = capacity\n        self._value = [neutral_element for _ in range(2 * capacity)]\n        self._operation = operation\n\n    def _reduce_helper(self, start, end, node, node_start, node_end):\n        if start == node_start and end == node_end:\n            return self._value[node]\n        mid = (node_start + node_end) // 2\n        if end <= mid:\n            return self._reduce_helper(start, end, 2 * node, node_start, mid)\n        else:\n            if mid + 1 <= start:\n                return self._reduce_helper(start, end, 2 * node + 1, mid + 1, node_end)\n            else:\n                return self._operation(\n                    self._reduce_helper(start, mid, 2 * node, node_start, mid),\n                    self._reduce_helper(mid + 1, end, 2 * node + 1, mid + 1, node_end)\n                )\n            \n    def reduce(self, start=0, end=None):\n        if end is None:\n            end = self._capacity\n        if end < 0:\n            end += self._capacity\n        end -= 1\n        return self._reduce_helper(start, end, 1, 0, self._capacity - 1)\n\n    def __setitem__(self, idx, val):\n        idx += self._capacity\n        self._value[idx] = val\n        idx //= 2\n        while idx >= 1:\n            self._value[idx] = self._operation( self._value[2 * idx], self._value[2 * idx + 1])\n            idx //= 2\n\n    def __getitem__(self, idx):\n        assert 0 <= idx < self._capacity\n        return self._value[self._capacity + idx]\n\nclass SumSegmentTree(SegmentTree):\n    def __init__(self, capacity):\n        super().__init__(capacity=capacity, operation=operator.add, neutral_element=0.0)\n\n    def sum(self, start=0, end=None):\n        return super().reduce(start, end)\n\n    def find_prefixsum_idx(self, prefixsum):\n        assert 0 <= prefixsum <= self.sum() + 1e-5\n        idx = 1\n        while idx < self._capacity:  # while non-leaf\n            if self._value[2 * idx] > prefixsum:\n                idx = 2 * idx\n            else:\n                prefixsum -= self._value[2 * idx]\n                idx = 2 * idx + 1\n        return idx - self._capacity\n\nclass MinSegmentTree(SegmentTree):\n    def __init__(self, capacity):\n        super().__init__(capacity=capacity, operation=min, neutral_element=float('inf'))\n\n    def min(self, start=0, end=None):\n        return super().reduce(start, end)\n    \n    \n### Priority Replay Buffer ###\nimport numpy as np\nimport random\n\nclass PriorityBuffer:\n    def __init__(self, size, alpha, beta):\n        self.max_size = size\n        self.alpha = alpha\n        self.beta = beta\n        \n        # Replay buffer storage\n        self.storage = []\n        self.next_idx = 0\n        \n        # Segment trees to manage priority\n        tree_size = 1\n        while tree_size <= size:\n            tree_size *= 2\n        self.sum_tree = SumSegmentTree(tree_size)\n        self.min_tree = MinSegmentTree(tree_size)\n        self.max_priority = 1.\n        \n    def append(self, experience):\n        \"Add an experience tuple to replay buffer.\"\n        self.sum_tree[self.next_idx] = self.max_priority ** self.alpha\n        self.min_tree[self.next_idx] = self.max_priority ** self.alpha\n        \n        if self.next_idx == len(self.storage):\n            self.storage.append(experience)\n        else:\n            self.storage[self.next_idx] = experience\n        self.next_idx = (self.next_idx + 1) % self.max_size\n        \n    def sample(self, batch_size):\n        \"Sample from replay buffer.\"\n        indices = self.select_indices_by_priority(batch_size)\n        weights = self.calculate_importance_weights(indices)\n        experiences = self.collect_experiences(indices)\n        return experiences, indices, weights\n        \n    def select_indices_by_priority(self, batch_size):\n        \"Select indices weighted by their relative priority.\"\n        indices = []\n        total_priority = self.sum_tree.sum(0, len(self.storage) - 1)\n        segment_size = total_priority / batch_size\n        for segment_idx in range(batch_size):\n            mass = random.random() * segment_size + segment_idx * segment_size\n            idx = self.sum_tree.find_prefixsum_idx(mass)\n            indices.append(idx)\n        return indices\n    \n    def calculate_importance_weights(self, indices):\n        \"Calculates importance weights which we scale losses for each sample by.\"\n        weights = []\n        min_priority = self.min_tree.min() / self.sum_tree.sum()\n        max_weight = (min_priority * len(self.storage)) ** (-self.beta)\n        for idx in indices:\n            priority = self.sum_tree[idx] / self.sum_tree.sum()\n            weight = (priority * len(self.storage)) ** (-self.beta)\n            weights.append(weight / max_weight)\n        weights = np.array(weights)\n        return weights\n    \n    def collect_experiences(self, indices):\n        \"Unpacks experiences into numpy arrays.\"\n        states, actions, rewards, next_states, dones = [], [], [], [], []\n        for idx in indices:\n            experience = self.storage[idx]\n            state, action, reward, next_state, done = experience\n            states.append(state)\n            actions.append(action)\n            rewards.append(reward)\n            next_states.append(next_state)\n            dones.append(done)\n        return torch.tensor(np.array(states), dtype=torch.float32), \\\n               torch.tensor(np.array(actions), dtype=torch.int64), \\\n               torch.tensor(np.array(rewards), dtype=torch.float32), \\\n               torch.tensor(np.array(next_states), dtype=torch.float32), \\\n               torch.tensor(np.array(dones), dtype=torch.float32)\n            \n    def update_priorities(self, indices, priorities):\n        \"Updates priorities at provided indices based on observed td-error.\"\n        assert len(indices) == len(priorities), \"Priorities and indices have different lengths.\"\n        for idx, priority in zip(indices, priorities):\n            assert priority > 0, \"Provided priority must be >= 0.\"\n            assert 0 <= idx < len(self.storage), \"Provided index out of bounds.\"\n            self.sum_tree[idx] = priority ** self.alpha\n            self.min_tree[idx] = priority ** self.alpha\n            self.max_priority = max(self.max_priority, priority)\n    \n    def __len__(self):\n        return len(self.storage)\n    \n    \n### Experience Replay Buffer ###\nclass ReplayBuffer:\n    def __init__(self, capacity, num_steps=1, gamma=0.99, prioritized=False, alpha=0.6, beta=0.4):\n        if prioritized:\n            self.buffer = PriorityBuffer(capacity, alpha, beta)\n        else:\n            self.buffer = deque(maxlen=capacity)\n        self.prioritized = prioritized\n        self.num_steps = num_steps\n        self.gamma = gamma\n        self.n_step_buffer = deque(maxlen=num_steps)\n        \n    def push(self, transition):\n        \"Pushes transition to buffer and handles n-step logic if required.\"\n        assert len(transition) == 6, \"Use new Gym step API: (s, a, r, s', ter, tru)\"\n        if self.num_steps == 1:\n            state, action, reward, next_state, terminated, _ = transition\n            self.buffer.append((state, action, reward, next_state, terminated))\n        else:\n            self.n_step_buffer.append(transition)\n            \n            # Calculate n-step reward\n            _, _, _, final_state, final_termination, final_truncation = transition\n            n_step_reward = 0.\n            for _, _, reward, _, _, _ in reversed(self.n_step_buffer):\n                  n_step_reward = n_step_reward * self.gamma + reward\n            state, action, _, _, _, _ = self.n_step_buffer[0]\n\n            # If n-step buffer is full, append to main buffer\n            if len(self.n_step_buffer) == self.num_steps:\n                self.buffer.append((state, action, n_step_reward, final_state, final_termination))\n            \n            # If done, clear n-step buffer\n            if final_termination or final_truncation:\n                self.n_step_buffer.clear()\n        \n    def sample(self, batch_size):\n        \"Samples a batch of experiences for learner to learn from.\"\n        if self.prioritized:\n            experiences, indices, weights = self.buffer.sample(batch_size)\n            states, actions, rewards, next_states, dones = experiences\n            return (states, actions, rewards, next_states, dones), indices, weights\n        else:\n            states, actions, rewards, next_states, dones = zip(*random.sample(self.buffer, batch_size))\n            states      = torch.tensor(np.stack(states),      dtype=torch.float32)\n            actions     = torch.tensor(actions,               dtype=torch.int64  )\n            rewards     = torch.tensor(rewards,               dtype=torch.float32)\n            next_states = torch.tensor(np.stack(next_states), dtype=torch.float32)\n            dones       = torch.tensor(dones,                 dtype=torch.float32)\n            return states, actions, rewards, next_states, dones\n        \n    def update_priorities(self, indices, priorities):\n        assert self.prioritized, \"Operation not available for non-prioritised buffers.\"\n        self.buffer.update_priorities(indices, priorities)\n        \n    def __len__(self):\n        return len(self.buffer)\n    \n    \n### Linear Scheduler ###\nclass LinearScheduler:\n    \"Used to create variables whose values are linearly annealed over time.\"\n    def __init__(self, start, end, total_duration, fraction=1.):\n        self.start = start\n        self.end = end\n        self.total_duration = total_duration\n        self.duration = int(total_duration * fraction)\n        self.step = 0\n        \n    def get(self):\n        \"Gets current value without incrementing step counter.\"\n        if self.step < self.duration:\n            current_value = self.start + (self.end - self.start) * (self.step / self.duration)\n        else:\n            current_value = self.end\n        return current_value\n\n    def __call__(self):\n        \"Gets current value and increments step counter.\"\n        current_value = self.get()\n        self.step += 1\n        return current_value    \n\n    \n### IQN Agent Class ###\nclass RIQN:\n    def __init__(self, config):\n        self.device = config['device']\n        self.env = gym.make(config['env_name'])\n        state_dim = np.prod(self.env.observation_space.shape)\n        action_dim = self.env.action_space.n\n        self.online_network = IQNetwork(state_dim, \n                                        action_dim, \n                                        config['hidden_dim'], \n                                        config['num_eval_quantiles'], \n                                        config['cosine_embedding_dim'],\n                                        config['noisy']).to(self.device)\n        self.target_network = IQNetwork(state_dim, \n                                        action_dim, \n                                        config['hidden_dim'], \n                                        config['num_eval_quantiles'], \n                                        config['cosine_embedding_dim'],\n                                        config['noisy']).to(self.device)\n        self.update_target_network(1.)\n        self.optimizer = torch.optim.AdamW(self.online_network.parameters(), lr=config['lr'])\n        self.buffer = ReplayBuffer(config['buffer_capacity'], config['num_steps'], config['gamma'], \n                                   config['per'], config['per_alpha'], config['per_beta'])\n        self.epsilon = LinearScheduler(config['eps_start'], config['eps_final'], \n                                       config['total_steps'], config['eps_fraction'])\n        self.config = config\n        \n    def update_target_network(self, tau):\n        \"Updates the parameters of the target network, tau controls how fully the weights are copied.\"\n        for target_param, online_param in zip(self.target_network.parameters(), self.online_network.parameters()):\n            target_param.data.copy_(tau * online_param.data + (1. - tau) * target_param.data)\n                \n    def select_action(self, state, epsilon):\n        \"Epsilon greedy action selection.\"\n        if random.random() < epsilon and not self.config['noisy']:\n            return self.env.action_space.sample()\n        state_tensor = torch.tensor(state, device=self.config['device']).unsqueeze(0)\n        return self.online_network(state_tensor).mean(-1).argmax().item()\n    \n    def learn(self):\n        # Load batch and create tensors\n        if self.config['per']:\n            experiences, indices, weights = self.buffer.sample(self.config['batch_size'])\n            states, actions, rewards, next_states, dones = experiences\n            weights = torch.tensor(weights, dtype=torch.float32, device=self.device).view(-1, 1, 1)\n        else:\n            states, actions, rewards, next_states, dones = self.buffer.sample(self.config['batch_size'])\n        states = states.to(self.device)\n        actions = actions.to(self.device).view(-1, 1, 1)\n        rewards = rewards.to(self.device).view(-1, 1, 1)\n        next_states = next_states.to(self.device)\n        dones = dones.to(self.device).view(-1, 1, 1)\n        \n        # Generate taus\n        taus = self.online_network.generate_taus(batch_size=self.config['batch_size'], uniform=False).to(self.config['device'])\n        \n        # Get number of quantiles from config\n        n_quantiles = self.config['num_eval_quantiles']\n        \n        # Predicted Q-value quantiles for current state\n        current_state_q_values = self.online_network(states, taus)\n        \n        # Gather Q-value quantiles of actions actually taken\n        current_action_q_values = torch.gather(current_state_q_values, dim=1, index=actions.expand(-1, -1, n_quantiles))\n        \n        # Compute targets\n        with torch.no_grad():\n            # Get best actions in next state with double DQN then gather Q-values with these actions\n            next_state_q_values     = self.target_network(next_states, taus)\n            next_state_best_actions = torch.argmax(self.online_network(next_states).mean(dim=2), dim=1, keepdims=True).unsqueeze(-1)\n            next_state_max_q_values = torch.gather(next_state_q_values, dim=1, index=next_state_best_actions.expand(-1, -1, n_quantiles))\n            \n            # Bellman equation to compute target Q-values for not done states\n            target_q_values = rewards + self.config['gamma'] ** self.config['num_steps'] * next_state_max_q_values * (1 - dones)\n        \n        # Calculate TD error and Quantile Huber loss\n        kappa = self.config['kappa']\n        td_error = target_q_values - current_action_q_values\n        huber_loss = torch.where(td_error.abs() <= kappa, \n                                 0.5 * td_error.pow(2), \n                                 kappa * (td_error.abs() - 0.5 * kappa))\n        quantile_loss = torch.abs(taus.unsqueeze(1) - (td_error < 0).float()) * huber_loss\n        \n        # Scale loss and update priorities if using PER\n        if self.config['per']:\n            quantile_loss = quantile_loss * weights\n            new_priorities = td_error.detach().abs().mean(dim=-1).squeeze(1).cpu().numpy()\n            self.buffer.update_priorities(indices, new_priorities)\n        \n        loss = quantile_loss.mean()\n        \n        # Backward pass\n        self.optimizer.zero_grad()\n        loss.backward()\n        nn.utils.clip_grad_norm_(self.online_network.parameters(), self.config['grad_norm_clip'])\n        self.optimizer.step()\n        \n        if self.config['noisy']:\n            self.online_network.reset_noise()\n            self.target_network.reset_noise()\n        \n    def train(self):\n        \"Trains agent for a given number of steps according to given configuration.\"\n        print(\"Training RIQN agent\\n\")\n            \n        # Logging information\n        logs = {'episode_count': 0, 'episodic_reward': 0., 'episode_rewards': [], 'start_time': time.time()}\n        \n        # Reset episode\n        state, _ = self.env.reset()\n        \n        # Main training loop\n        for step in range(1, self.config['total_steps'] + 1):\n            # Get action and execute in envrionment\n            action = self.select_action(state, self.epsilon())\n            next_state, reward, terminated, truncated, _ = self.env.step(action)\n            \n            # Update logs\n            logs['episodic_reward'] += reward\n            \n            # Push experience to buffer\n            self.buffer.push((state, action, reward, next_state, terminated, truncated))\n\n            if terminated or truncated:\n                state, _ = self.env.reset()\n                \n                # Update logs\n                logs['episode_count'] += 1\n                logs['episode_rewards'].append(logs['episodic_reward'])\n                logs['episodic_reward'] = 0.\n            else:\n                state = next_state\n            \n            # Perform learning step\n            if len(self.buffer) >= self.config['batch_size'] and step > self.config['learning_starts']:\n                self.learn()\n            \n            # Update target network\n            if step % self.config['target_update'] == 0:\n                self.update_target_network(self.config['tau'])\n                \n            # If mean of last 20 rewards exceed target, end training\n            if len(logs['episode_rewards']) > 0 and np.mean(logs['episode_rewards'][-20:]) >= self.config['target_reward']:\n                break\n            \n            # Print training info if verbose\n            if self.config['verbose'] and step % 100 == 0 and len(logs['episode_rewards']) > 0:\n                print(f\"\\r--- {100 * step / self.config['total_steps']:.1f}%\" \n                      f\"\\t Step: {step:,}\"\n                      f\"\\t Mean Reward: {np.mean(logs['episode_rewards'][-20:]):.2f}\"\n                      f\"\\t Epsilon: {(1-self.config['noisy']) * self.epsilon.get():.2f}\"\n                      f\"\\t Episode: {logs['episode_count']:,}\"\n                      f\"\\t Duration: {time.time() - logs['start_time']:,.1f}s  ---\", end='')\n                if step % 10000 == 0:\n                    print()\n                    \n        # Training ended\n        print(\"\\n\\nTraining done\")\n        logs['end_time'] = time.time()\n        logs['duration'] = logs['end_time'] - logs['start_time']\n        return logs\n    \n        \n### IQN Configuration ###\nriqn_config = {\n    'env_name'            : 'LunarLander-v2',  # Gym environment to use\n    'device'              :         'cpu',  # Device used for learning\n    'total_steps'         :        1000000,  # Total training steps\n    'hidden_dim'          :            64,  # Number of neurons in Q-network hidden layer\n    'batch_size'          :            64,  # Number of experience tuples sampled per learning update\n    'noisy'               :          True,  # Noisy networks tweak\n    'per'                 :          True,  # Use prioritised experience replay\n    'per_alpha'           :           0.6,  # Priority sampling weight (0 for maximum sampling entropy)\n    'per_beta'            :           0.4,  # Importance sampling correction to reduce bias from frequently sampled experiences\n    'buffer_capacity'     :        100000,  # Maximum length of replay buffer\n    'target_update'       :            20,  # How often to perform target network weight synchronisations\n    'tau'                 :           0.5,  # When copying online network weights to target network, what weight is given to online network weights\n    'eps_start'           :           0.8,  # Initial epsilon to use\n    'eps_final'           :          0.05,  # Lowest possible epsilon value\n    'eps_fraction'        :           0.6,  # Fraction of entire training period over which the exploration rate is reduced\n    'learning_starts'     :           512,  # Step to begin learning at\n    'train_frequency'     :             1,  # Performs a learning update every `train_frequency` steps\n    'lr'                  :          5e-4,  # Learning rate\n    'grad_norm_clip'      :            10,  # Global grad norm clipping\n    'gamma'               :          0.99,  # Discount factor\n    'num_steps'           :             3,  # Multistep reward steps\n    'kappa'               :           1.0,  # Huber loss kappa\n    'num_eval_quantiles'  :            16,  # N in the IQN paper, resolution of the quantile distribution used\n    'cosine_embedding_dim':            16,  # N' in IQN paper, dimensionality of cosine embeddings generated\n    'target_reward'       :           195,  # If set to a number, training will stop when mean reward for recent episodes exceeds this\n    'verbose'             :          True,  # Prints steps and rewards in output\n}","metadata":{},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"agent = RIQN(riqn_config)\nlogs = agent.train()","metadata":{},"execution_count":4,"outputs":[{"name":"stdout","output_type":"stream","text":"Training RIQN agent\n\n\n\n--- 1.0%\t Step: 10,000\t Mean Reward: -269.66\t Epsilon: 0.00\t Episode: 23\t Duration: 209.9s  ---\n\n--- 2.0%\t Step: 20,000\t Mean Reward: -103.68\t Epsilon: 0.00\t Episode: 34\t Duration: 426.2s  ---\n\n--- 3.0%\t Step: 30,000\t Mean Reward: -111.00\t Epsilon: 0.00\t Episode: 47\t Duration: 641.5s  ---\n\n--- 4.0%\t Step: 40,000\t Mean Reward: -109.47\t Epsilon: 0.00\t Episode: 58\t Duration: 872.1s  ---\n\n--- 5.0%\t Step: 50,000\t Mean Reward: -102.97\t Epsilon: 0.00\t Episode: 70\t Duration: 1,093.4s  ---\n\n--- 6.0%\t Step: 60,000\t Mean Reward: -76.22\t Epsilon: 0.00\t Episode: 83\t Duration: 1,302.1s  ----\n\n--- 7.0%\t Step: 70,000\t Mean Reward: -47.50\t Epsilon: 0.00\t Episode: 93\t Duration: 1,516.7s  ---\n\n--- 8.0%\t Step: 80,000\t Mean Reward: -11.23\t Epsilon: 0.00\t Episode: 104\t Duration: 1,723.4s  ---\n\n--- 9.0%\t Step: 90,000\t Mean Reward: 2.45\t Epsilon: 0.00\t Episode: 116\t Duration: 1,922.2s  -----\n\n--- 10.0%\t Step: 100,000\t Mean Reward: 32.17\t Epsilon: 0.00\t Episode: 126\t Duration: 2,146.0s  ---\n\n--- 11.0%\t Step: 110,000\t Mean Reward: 15.25\t Epsilon: 0.00\t Episode: 138\t Duration: 2,392.0s  ---\n\n--- 12.0%\t Step: 120,000\t Mean Reward: 16.59\t Epsilon: 0.00\t Episode: 152\t Duration: 2,585.8s  ---\n\n--- 13.0%\t Step: 130,000\t Mean Reward: -29.18\t Epsilon: 0.00\t Episode: 166\t Duration: 2,786.0s  ---\n\n--- 14.0%\t Step: 140,000\t Mean Reward: 14.90\t Epsilon: 0.00\t Episode: 181\t Duration: 2,961.7s  ----\n\n--- 15.0%\t Step: 150,000\t Mean Reward: 34.28\t Epsilon: 0.00\t Episode: 193\t Duration: 3,132.4s  ---\n\n--- 16.0%\t Step: 160,000\t Mean Reward: 22.54\t Epsilon: 0.00\t Episode: 208\t Duration: 3,308.2s  ---\n\n--- 17.0%\t Step: 170,000\t Mean Reward: -1.84\t Epsilon: 0.00\t Episode: 226\t Duration: 3,484.4s  ---\n\n--- 18.0%\t Step: 180,000\t Mean Reward: 12.56\t Epsilon: 0.00\t Episode: 238\t Duration: 3,660.8s  ----\n\n--- 19.0%\t Step: 190,000\t Mean Reward: 46.72\t Epsilon: 0.00\t Episode: 252\t Duration: 3,834.7s  ---\n\n--- 20.0%\t Step: 200,000\t Mean Reward: 22.20\t Epsilon: 0.00\t Episode: 267\t Duration: 4,012.9s  ---\n\n--- 21.0%\t Step: 210,000\t Mean Reward: 47.27\t Epsilon: 0.00\t Episode: 285\t Duration: 4,191.8s  ---\n\n--- 22.0%\t Step: 220,000\t Mean Reward: 112.34\t Epsilon: 0.00\t Episode: 299\t Duration: 4,367.1s  ---\n\n--- 23.0%\t Step: 230,000\t Mean Reward: 90.86\t Epsilon: 0.00\t Episode: 316\t Duration: 4,557.6s  ----\n\n--- 24.0%\t Step: 240,000\t Mean Reward: 87.27\t Epsilon: 0.00\t Episode: 332\t Duration: 4,786.5s  ---\n\n--- 25.0%\t Step: 250,000\t Mean Reward: 69.81\t Epsilon: 0.00\t Episode: 349\t Duration: 5,393.5s  ---\n\n--- 26.0%\t Step: 260,000\t Mean Reward: 90.04\t Epsilon: 0.00\t Episode: 368\t Duration: 6,018.2s  ----\n\n--- 27.0%\t Step: 270,000\t Mean Reward: 50.75\t Epsilon: 0.00\t Episode: 389\t Duration: 6,639.6s  ---\n\n--- 28.0%\t Step: 280,000\t Mean Reward: 123.51\t Epsilon: 0.00\t Episode: 409\t Duration: 7,252.6s  ---\n\n--- 29.0%\t Step: 290,000\t Mean Reward: 89.54\t Epsilon: 0.00\t Episode: 424\t Duration: 7,873.8s  ----\n\n--- 30.0%\t Step: 300,000\t Mean Reward: 56.67\t Epsilon: 0.00\t Episode: 441\t Duration: 8,483.9s  ---\n\n--- 31.0%\t Step: 310,000\t Mean Reward: 15.62\t Epsilon: 0.00\t Episode: 463\t Duration: 8,835.0s  ----\n\n--- 32.0%\t Step: 320,000\t Mean Reward: 45.24\t Epsilon: 0.00\t Episode: 481\t Duration: 9,011.1s  ---\n\n--- 33.0%\t Step: 330,000\t Mean Reward: 30.31\t Epsilon: 0.00\t Episode: 500\t Duration: 9,190.9s  ---\n\n--- 34.0%\t Step: 340,000\t Mean Reward: 68.68\t Epsilon: 0.00\t Episode: 516\t Duration: 9,364.0s  ----\n\n--- 35.0%\t Step: 350,000\t Mean Reward: 109.96\t Epsilon: 0.00\t Episode: 530\t Duration: 9,545.5s  ---\n\n--- 36.0%\t Step: 360,000\t Mean Reward: 70.67\t Epsilon: 0.00\t Episode: 546\t Duration: 9,724.4s  ----\n\n--- 37.0%\t Step: 370,000\t Mean Reward: 66.01\t Epsilon: 0.00\t Episode: 563\t Duration: 9,901.9s  ---\n\n--- 38.0%\t Step: 380,000\t Mean Reward: 57.11\t Epsilon: 0.00\t Episode: 580\t Duration: 10,084.8s  ---\n\n--- 39.0%\t Step: 390,000\t Mean Reward: -20.06\t Epsilon: 0.00\t Episode: 601\t Duration: 10,264.1s  ---\n\n--- 40.0%\t Step: 400,000\t Mean Reward: 111.09\t Epsilon: 0.00\t Episode: 618\t Duration: 10,441.6s  ---\n\n--- 41.0%\t Step: 410,000\t Mean Reward: 127.68\t Epsilon: 0.00\t Episode: 637\t Duration: 10,621.2s  ---\n\n--- 42.0%\t Step: 420,000\t Mean Reward: 81.59\t Epsilon: 0.00\t Episode: 654\t Duration: 10,800.7s  ----\n\n--- 43.0%\t Step: 430,000\t Mean Reward: 63.82\t Epsilon: 0.00\t Episode: 676\t Duration: 10,981.2s  ----\n\n--- 44.0%\t Step: 440,000\t Mean Reward: 44.40\t Epsilon: 0.00\t Episode: 698\t Duration: 11,161.8s  ----\n\n--- 45.0%\t Step: 450,000\t Mean Reward: 67.21\t Epsilon: 0.00\t Episode: 716\t Duration: 11,340.3s  ---\n\n--- 46.0%\t Step: 460,000\t Mean Reward: 106.34\t Epsilon: 0.00\t Episode: 731\t Duration: 11,519.0s  ---\n\n--- 47.0%\t Step: 470,000\t Mean Reward: 54.25\t Epsilon: 0.00\t Episode: 751\t Duration: 11,701.1s  ----\n\n--- 48.0%\t Step: 480,000\t Mean Reward: 70.49\t Epsilon: 0.00\t Episode: 768\t Duration: 11,889.7s  ---\n\n--- 49.0%\t Step: 490,000\t Mean Reward: 52.20\t Epsilon: 0.00\t Episode: 787\t Duration: 12,078.5s  ---\n\n--- 50.0%\t Step: 500,000\t Mean Reward: 6.31\t Epsilon: 0.00\t Episode: 808\t Duration: 12,271.4s  ----\n\n--- 51.0%\t Step: 510,000\t Mean Reward: 122.20\t Epsilon: 0.00\t Episode: 823\t Duration: 12,452.0s  ---\n\n--- 52.0%\t Step: 520,000\t Mean Reward: 44.31\t Epsilon: 0.00\t Episode: 842\t Duration: 12,631.9s  ----\n\n--- 53.0%\t Step: 530,000\t Mean Reward: 18.94\t Epsilon: 0.00\t Episode: 860\t Duration: 12,810.8s  ---\n\n--- 54.0%\t Step: 540,000\t Mean Reward: 31.85\t Epsilon: 0.00\t Episode: 881\t Duration: 12,991.2s  ---\n\n--- 55.0%\t Step: 550,000\t Mean Reward: 55.16\t Epsilon: 0.00\t Episode: 900\t Duration: 13,168.0s  ---\n\n--- 56.0%\t Step: 560,000\t Mean Reward: 80.28\t Epsilon: 0.00\t Episode: 915\t Duration: 13,345.8s  ---\n\n--- 57.0%\t Step: 570,000\t Mean Reward: -6.15\t Epsilon: 0.00\t Episode: 936\t Duration: 13,520.8s  ---\n\n--- 58.0%\t Step: 580,000\t Mean Reward: 44.97\t Epsilon: 0.00\t Episode: 953\t Duration: 13,700.3s  ---\n\n--- 59.0%\t Step: 590,000\t Mean Reward: 56.29\t Epsilon: 0.00\t Episode: 969\t Duration: 13,877.5s  ---\n\n--- 60.0%\t Step: 600,000\t Mean Reward: 55.29\t Epsilon: 0.00\t Episode: 985\t Duration: 14,056.9s  ---\n\n--- 61.0%\t Step: 610,000\t Mean Reward: 46.12\t Epsilon: 0.00\t Episode: 1,002\t Duration: 14,235.1s  ---\n\n--- 62.0%\t Step: 620,000\t Mean Reward: 47.07\t Epsilon: 0.00\t Episode: 1,017\t Duration: 14,413.2s  ---\n\n--- 63.0%\t Step: 630,000\t Mean Reward: 89.75\t Epsilon: 0.00\t Episode: 1,036\t Duration: 14,591.6s  ----\n\n--- 64.0%\t Step: 640,000\t Mean Reward: 33.90\t Epsilon: 0.00\t Episode: 1,058\t Duration: 14,767.5s  ----\n\n--- 65.0%\t Step: 650,000\t Mean Reward: 127.74\t Epsilon: 0.00\t Episode: 1,078\t Duration: 14,948.4s  ---\n\n--- 66.0%\t Step: 660,000\t Mean Reward: 133.35\t Epsilon: 0.00\t Episode: 1,093\t Duration: 15,124.3s  ---\n\n--- 67.0%\t Step: 670,000\t Mean Reward: 75.80\t Epsilon: 0.00\t Episode: 1,113\t Duration: 15,309.7s  ----\n\n--- 68.0%\t Step: 680,000\t Mean Reward: 146.10\t Epsilon: 0.00\t Episode: 1,130\t Duration: 15,498.2s  ---\n\n--- 69.0%\t Step: 690,000\t Mean Reward: 98.95\t Epsilon: 0.00\t Episode: 1,149\t Duration: 15,678.9s  ----\n\n--- 70.0%\t Step: 700,000\t Mean Reward: 103.60\t Epsilon: 0.00\t Episode: 1,165\t Duration: 15,854.1s  ---\n\n--- 71.0%\t Step: 710,000\t Mean Reward: 129.97\t Epsilon: 0.00\t Episode: 1,182\t Duration: 16,036.1s  ---\n\n--- 72.0%\t Step: 720,000\t Mean Reward: 108.40\t Epsilon: 0.00\t Episode: 1,201\t Duration: 16,215.5s  ---\n\n--- 73.0%\t Step: 730,000\t Mean Reward: 118.53\t Epsilon: 0.00\t Episode: 1,218\t Duration: 16,392.8s  ---\n\n--- 74.0%\t Step: 740,000\t Mean Reward: 124.22\t Epsilon: 0.00\t Episode: 1,237\t Duration: 16,569.2s  ---\n\n--- 75.0%\t Step: 750,000\t Mean Reward: 123.89\t Epsilon: 0.00\t Episode: 1,256\t Duration: 16,748.6s  ---\n\n--- 76.0%\t Step: 760,000\t Mean Reward: 175.78\t Epsilon: 0.00\t Episode: 1,273\t Duration: 16,927.3s  ---\n\n--- 77.0%\t Step: 770,000\t Mean Reward: 105.77\t Epsilon: 0.00\t Episode: 1,294\t Duration: 17,107.9s  ---\n\n--- 78.0%\t Step: 780,000\t Mean Reward: 135.80\t Epsilon: 0.00\t Episode: 1,314\t Duration: 17,287.8s  ---\n\n--- 79.0%\t Step: 790,000\t Mean Reward: 96.17\t Epsilon: 0.00\t Episode: 1,334\t Duration: 17,466.6s  ----\n\n--- 80.0%\t Step: 800,000\t Mean Reward: 114.09\t Epsilon: 0.00\t Episode: 1,356\t Duration: 17,645.2s  ---\n\n--- 81.0%\t Step: 810,000\t Mean Reward: 48.53\t Epsilon: 0.00\t Episode: 1,379\t Duration: 17,825.3s  ----\n\n--- 82.0%\t Step: 820,000\t Mean Reward: 98.25\t Epsilon: 0.00\t Episode: 1,399\t Duration: 18,003.3s  ---\n\n--- 83.0%\t Step: 830,000\t Mean Reward: 85.62\t Epsilon: 0.00\t Episode: 1,421\t Duration: 18,184.9s  ---\n\n--- 84.0%\t Step: 840,000\t Mean Reward: 153.45\t Epsilon: 0.00\t Episode: 1,439\t Duration: 18,360.9s  ---\n\n--- 85.0%\t Step: 850,000\t Mean Reward: 167.85\t Epsilon: 0.00\t Episode: 1,461\t Duration: 18,542.9s  ---\n\n--- 86.0%\t Step: 860,000\t Mean Reward: 171.46\t Epsilon: 0.00\t Episode: 1,483\t Duration: 18,725.0s  ---\n\n--- 87.0%\t Step: 870,000\t Mean Reward: 149.38\t Epsilon: 0.00\t Episode: 1,501\t Duration: 18,913.3s  ---\n\n--- 88.0%\t Step: 880,000\t Mean Reward: 139.49\t Epsilon: 0.00\t Episode: 1,520\t Duration: 19,105.3s  ---\n\n--- 89.0%\t Step: 890,000\t Mean Reward: 157.98\t Epsilon: 0.00\t Episode: 1,536\t Duration: 19,299.1s  ---\n\n--- 90.0%\t Step: 900,000\t Mean Reward: 167.73\t Epsilon: 0.00\t Episode: 1,551\t Duration: 19,480.0s  ---\n\n--- 91.0%\t Step: 910,000\t Mean Reward: 174.50\t Epsilon: 0.00\t Episode: 1,571\t Duration: 19,659.5s  ---\n\n--- 91.1%\t Step: 911,400\t Mean Reward: 191.68\t Epsilon: 0.00\t Episode: 1,573\t Duration: 19,683.3s  ---\n\n\n\nTraining done\n"}]}]}