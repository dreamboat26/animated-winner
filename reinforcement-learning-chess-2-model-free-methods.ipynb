{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Chess \n",
    "Reinforcement Learning Chess is a series of notebooks where I implement Reinforcement Learning algorithms to develop a chess AI. I start of with simpler versions (environments) that can be tackled with simple methods and gradually expand on those concepts untill I have a full-flegded chess AI. \n",
    "\n",
    "[**Notebook 1: Policy Iteration**](https://www.kaggle.com/arjanso/reinforcement-learning-chess-1-policy-iteration)  \n",
    "[**Notebook 3: Q-networks**](https://www.kaggle.com/arjanso/reinforcement-learning-chess-3-q-networks)  \n",
    "[**Notebook 4: Policy Gradients**](https://www.kaggle.com/arjanso/reinforcement-learning-chess-4-policy-gradients)  \n",
    "[**Notebook 5: Monte Carlo Tree Search**](https://www.kaggle.com/arjanso/reinforcement-learning-chess-5-tree-search)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook II: Model-free control\n",
    "In this notebook I use the same move-chess environment as in notebook 1. In this notebook I mentioned that policy evaluation calculates the state value by backing up the successor state values and the transition probabilities to those states. The problem is that these probabilities are usually unknown in real-world problems. Luckily there are control techniques that can work in these unknown environments. These techniques don't leverage any prior knowledge about the environment's dynamics, they are model-free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/arjangroen/RLC.git\r\n",
      "  Cloning https://github.com/arjangroen/RLC.git to /tmp/pip-req-build-gn6xg08b\r\n",
      "  Running command git clone -q https://github.com/arjangroen/RLC.git /tmp/pip-req-build-gn6xg08b\r\n",
      "Building wheels for collected packages: RLC\r\n",
      "  Building wheel for RLC (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-x011tmyw/wheels/04/68/a5/cb835cd3d76a49de696a942739c71a56bfe66d0d8ea7b4b446\r\n",
      "Successfully built RLC\r\n",
      "Installing collected packages: RLC\r\n",
      "Successfully installed RLC-0.3\r\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade git+https://github.com/arjangroen/RLC.git  # RLC is the Reinforcement Learning package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RLC.move_chess.environment import Board\n",
    "from RLC.move_chess.agent import Piece\n",
    "from RLC.move_chess.learn import Reinforce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The environment\n",
    "- The state space is a 8 by 8 grid\n",
    "- The starting state S is the top-left square (0,0)\n",
    "- The terminal state F is square (5,7). \n",
    "- Every move from state to state gives a reward of minus 1\n",
    "- Naturally the best policy for this evironment is to move from S to F in the lowest amount of moves possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[S]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]'],\n",
       " ['[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]'],\n",
       " ['[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]'],\n",
       " ['[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]'],\n",
       " ['[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]'],\n",
       " ['[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]'],\n",
       " ['[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[ ]'],\n",
       " ['[ ]', '[ ]', '[ ]', '[ ]', '[ ]', '[F]', '[ ]', '[ ]']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = Board()\n",
    "env.render()\n",
    "env.visual_board"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The agent\n",
    "- The agent is a chess Piece (king, queen, rook, knight or bishop)\n",
    "- The agent has a behavior policy determining what the agent does in what state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Piece(piece='king')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforce\n",
    "- The reinforce object contains the algorithms for solving move chess\n",
    "- The agent and the environment are attributes of the Reinforce object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = Reinforce(p,env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Monte Carlo Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theory**  \n",
    "The basic intuition is:\n",
    "* We do not know the environment, so we sample an episode from beginning to end by running our current policy\n",
    "* We try to estimate the action-values rather than the state values. This is because we are working model-free so just knowning state values won't help us select the best actions. \n",
    "* The value of a state-action value is defined as the future returns from the first visit of that state-action\n",
    "* Based on this we can improve our policy and repeat the process untill the algorithm converges\n",
    "\n",
    "![](http://incompleteideas.net/book/first/ebook/pseudotmp5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def monte_carlo_learning(self, epsilon=0.1):\n",
      "        \"\"\"\n",
      "        Learn move chess through monte carlo control\n",
      "        :param epsilon: exploration rate\n",
      "        :return:\n",
      "        \"\"\"\n",
      "        state = (0, 0)\n",
      "        self.env.state = state\n",
      "\n",
      "        # Play out an episode\n",
      "        states, actions, rewards = self.play_episode(state, epsilon=epsilon)\n",
      "\n",
      "        first_visits = []\n",
      "        for idx, state in enumerate(states):\n",
      "            action_index = actions[idx]\n",
      "            if (state, action_index) in first_visits:\n",
      "                continue\n",
      "            r = np.sum(rewards[idx:])\n",
      "            if (state, action_index) in self.agent.Returns.keys():\n",
      "                self.agent.Returns[(state, action_index)].append(r)\n",
      "            else:\n",
      "                self.agent.Returns[(state, action_index)] = [r]\n",
      "            self.agent.action_function[state[0], state[1], action_index] = \\\n",
      "                np.mean(self.agent.Returns[(state, action_index)])\n",
      "            first_visits.append((state, action_index))\n",
      "        # Update the policy. In Monte Carlo Control, this is greedy behavior with respect to the action function\n",
      "        self.agent.policy = self.agent.action_function.copy()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(r.monte_carlo_learning))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Demo**  \n",
    "We do 100 iterations of monte carlo learning while maintaining a high exploration rate of 0.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(100):\n",
    "    eps = 0.5\n",
    "    r.monte_carlo_learning(epsilon=eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['↙', '↖', '↓', '←', '↗', '→', '→', '↗'],\n",
      " ['←', '↓', '↙', '↓', '↖', '←', '↘', '↗'],\n",
      " ['↑', '↙', '↖', '↙', '→', '↖', '→', '→'],\n",
      " ['→', '↙', '↙', '↗', '←', '→', '↙', '↘'],\n",
      " ['↓', '↙', '↓', '←', '↙', '↖', '↗', '↑'],\n",
      " ['↓', '↘', '→', '→', '↙', '↙', '↘', '↗'],\n",
      " ['↗', '↓', '↘', '↖', '↘', '↓', '↙', '←'],\n",
      " ['↗', '→', '↖', '→', '→', 'F', '↗', '←']]\n"
     ]
    }
   ],
   "source": [
    "r.visualize_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best action value for each state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -52,  -54,  -51,  -55,  -98,  -76,  -70,  -63],\n",
       "       [ -47,  -47,  -47,  -56,  -77, -140,  -81,  -58],\n",
       "       [ -57,  -39,  -44,  -43,  -85,  -65, -170, -174],\n",
       "       [ -32,  -31,  -32,  -59,  -66, -105,  -63,  -84],\n",
       "       [ -30,  -34,  -37,  -36,  -39,  -62, -146,  -44],\n",
       "       [ -26,  -16,  -25,  -24,  -31,  -57,  -65,  -22],\n",
       "       [ -23,  -32,  -12,  -24,   -1,   -1,   -1,   -3],\n",
       "       [ -35,  -30,  -32,   -4,   -1,    0,    0,   -2]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.agent.action_function.max(axis=2).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Temporal Difference Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theory**\n",
    "* Like Policy Iteration, we can back up state-action values from the successor state action without waiting for the episode to end. \n",
    "* We update our state-action value in the direction of the successor state action value.\n",
    "* The algorithm is called SARSA: State-Action-Reward-State-Action.\n",
    "* Epsilon is gradually lowered (the GLIE property)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def sarsa_td(self, n_episodes=1000, alpha=0.01, gamma=0.9):\n",
      "        \"\"\"\n",
      "        Run the sarsa control algorithm (TD0), finding the optimal policy and action function\n",
      "        :param n_episodes: int, amount of episodes to train\n",
      "        :param alpha: learning rate\n",
      "        :param gamma: discount factor of future rewards\n",
      "        :return: finds the optimal policy for move chess\n",
      "        \"\"\"\n",
      "        for k in range(n_episodes):\n",
      "            state = (0, 0)\n",
      "            self.env.state = state\n",
      "            episode_end = False\n",
      "            epsilon = max(1 / (1 + k), 0.05)\n",
      "            while not episode_end:\n",
      "                state = self.env.state\n",
      "                action_index = self.agent.apply_policy(state, epsilon)\n",
      "                action = self.agent.action_space[action_index]\n",
      "                reward, episode_end = self.env.step(action)\n",
      "                successor_state = self.env.state\n",
      "                successor_action_index = self.agent.apply_policy(successor_state, epsilon)\n",
      "\n",
      "                action_value = self.agent.action_function[state[0], state[1], action_index]\n",
      "                successor_action_value = self.agent.action_function[successor_state[0],\n",
      "                                                                    successor_state[1], successor_action_index]\n",
      "\n",
      "                q_update = alpha * (reward + gamma * successor_action_value - action_value)\n",
      "\n",
      "                self.agent.action_function[state[0], state[1], action_index] += q_update\n",
      "                self.agent.policy = self.agent.action_function.copy()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(r.sarsa_td))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Demonstration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Piece(piece='king')\n",
    "env = Board()\n",
    "r = Reinforce(p,env)\n",
    "r.sarsa_td(n_episodes=10000,alpha=0.2,gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['↘', '↘', '↘', '→', '↘', '→', '↑', '↗'],\n",
      " ['↘', '↙', '↘', '↙', '→', '↘', '→', '↓'],\n",
      " ['↘', '↘', '↓', '↓', '↙', '↖', '↙', '↖'],\n",
      " ['↘', '↘', '↓', '↙', '↘', '↘', '↑', '↙'],\n",
      " ['→', '↘', '↘', '↘', '↘', '↓', '↘', '↙'],\n",
      " ['↘', '→', '↘', '↘', '↘', '↘', '↓', '↙'],\n",
      " ['←', '↗', '↘', '↘', '↘', '↓', '↙', '←'],\n",
      " ['↗', '→', '→', '→', '→', 'F', '←', '←']]\n"
     ]
    }
   ],
   "source": [
    "r.visualize_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 TD-lambda\n",
    "**Theory**  \n",
    "In Monte Carlo we do a full-depth backup while in Temporal Difference Learning we de a 1-step backup. You could also choose a depth in-between: backup by n steps. But what value to choose for n?\n",
    "* TD lambda uses all n-steps and discounts them with factor lambda\n",
    "* This is called lambda-returns\n",
    "* TD-lambda uses an eligibility-trace to keep track of the previously encountered states\n",
    "* This way action-values can be updated in retrospect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def sarsa_lambda(self, n_episodes=1000, alpha=0.05, gamma=0.9, lamb=0.8):\n",
      "        \"\"\"\n",
      "        Run the sarsa control algorithm (TD lambda), finding the optimal policy and action function\n",
      "        :param n_episodes: int, amount of episodes to train\n",
      "        :param alpha: learning rate\n",
      "        :param gamma: discount factor of future rewards\n",
      "        :param lamb: lambda parameter describing the decay over n-step returns\n",
      "        :return: finds the optimal move chess policy\n",
      "        \"\"\"\n",
      "        for k in range(n_episodes):\n",
      "            self.agent.E = np.zeros(shape=self.agent.action_function.shape)\n",
      "            state = (0, 0)\n",
      "            self.env.state = state\n",
      "            episode_end = False\n",
      "            epsilon = max(1 / (1 + k), 0.2)\n",
      "            action_index = self.agent.apply_policy(state, epsilon)\n",
      "            action = self.agent.action_space[action_index]\n",
      "            while not episode_end:\n",
      "                reward, episode_end = self.env.step(action)\n",
      "                successor_state = self.env.state\n",
      "                successor_action_index = self.agent.apply_policy(successor_state, epsilon)\n",
      "\n",
      "                action_value = self.agent.action_function[state[0], state[1], action_index]\n",
      "                if not episode_end:\n",
      "                    successor_action_value = self.agent.action_function[successor_state[0],\n",
      "                                                                        successor_state[1], successor_action_index]\n",
      "                else:\n",
      "                    successor_action_value = 0\n",
      "                delta = reward + gamma * successor_action_value - action_value\n",
      "                self.agent.E[state[0], state[1], action_index] += 1\n",
      "                self.agent.action_function = self.agent.action_function + alpha * delta * self.agent.E\n",
      "                self.agent.E = gamma * lamb * self.agent.E\n",
      "                state = successor_state\n",
      "                action = self.agent.action_space[successor_action_index]\n",
      "                action_index = successor_action_index\n",
      "                self.agent.policy = self.agent.action_function.copy()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(r.sarsa_lambda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Demonstration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Piece(piece='king')\n",
    "env = Board()\n",
    "r = Reinforce(p,env)\n",
    "r.sarsa_lambda(n_episodes=10000,alpha=0.2,gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['↓', '↘', '↘', '↓', '↓', '↙', '↓', '→'],\n",
      " ['↘', '↘', '↘', '↓', '↙', '↑', '→', '↙'],\n",
      " ['↘', '↓', '↘', '↘', '↘', '↓', '↙', '↓'],\n",
      " ['→', '↘', '↘', '↓', '↓', '↓', '←', '↙'],\n",
      " ['↘', '↘', '↘', '↘', '↓', '↘', '↓', '↙'],\n",
      " ['↘', '↘', '↘', '↘', '↓', '↓', '↙', '↙'],\n",
      " ['↘', '→', '↘', '↘', '↘', '↓', '↙', '↙'],\n",
      " ['→', '→', '↗', '→', '→', 'F', '←', '←']]\n"
     ]
    }
   ],
   "source": [
    "r.visualize_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theory**\n",
    "* In SARSA/TD0, we back-up our action values with the succesor action value\n",
    "* In SARSA-max/Q learning, we back-up using the maximum action value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def sarsa_lambda(self, n_episodes=1000, alpha=0.05, gamma=0.9, lamb=0.8):\n",
      "        \"\"\"\n",
      "        Run the sarsa control algorithm (TD lambda), finding the optimal policy and action function\n",
      "        :param n_episodes: int, amount of episodes to train\n",
      "        :param alpha: learning rate\n",
      "        :param gamma: discount factor of future rewards\n",
      "        :param lamb: lambda parameter describing the decay over n-step returns\n",
      "        :return: finds the optimal move chess policy\n",
      "        \"\"\"\n",
      "        for k in range(n_episodes):\n",
      "            self.agent.E = np.zeros(shape=self.agent.action_function.shape)\n",
      "            state = (0, 0)\n",
      "            self.env.state = state\n",
      "            episode_end = False\n",
      "            epsilon = max(1 / (1 + k), 0.2)\n",
      "            action_index = self.agent.apply_policy(state, epsilon)\n",
      "            action = self.agent.action_space[action_index]\n",
      "            while not episode_end:\n",
      "                reward, episode_end = self.env.step(action)\n",
      "                successor_state = self.env.state\n",
      "                successor_action_index = self.agent.apply_policy(successor_state, epsilon)\n",
      "\n",
      "                action_value = self.agent.action_function[state[0], state[1], action_index]\n",
      "                if not episode_end:\n",
      "                    successor_action_value = self.agent.action_function[successor_state[0],\n",
      "                                                                        successor_state[1], successor_action_index]\n",
      "                else:\n",
      "                    successor_action_value = 0\n",
      "                delta = reward + gamma * successor_action_value - action_value\n",
      "                self.agent.E[state[0], state[1], action_index] += 1\n",
      "                self.agent.action_function = self.agent.action_function + alpha * delta * self.agent.E\n",
      "                self.agent.E = gamma * lamb * self.agent.E\n",
      "                state = successor_state\n",
      "                action = self.agent.action_space[successor_action_index]\n",
      "                action_index = successor_action_index\n",
      "                self.agent.policy = self.agent.action_function.copy()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(r.sarsa_lambda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Demonstration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Piece(piece='king')\n",
    "env = Board()\n",
    "r = Reinforce(p,env)\n",
    "r.q_learning(n_episodes=1000,alpha=0.2,gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['↘', '↑', '↖', '→', '→', '↗', '↑', '↖'],\n",
      " ['↘', '↘', '↘', '↑', '↗', '↘', '↘', '↗'],\n",
      " ['↘', '↓', '↘', '↘', '↘', '↖', '↖', '↘'],\n",
      " ['→', '↘', '↓', '↘', '↙', '↙', '↗', '↖'],\n",
      " ['↖', '↘', '↘', '↘', '↙', '↓', '↑', '↓'],\n",
      " ['↘', '↘', '→', '↘', '↓', '↙', '↙', '↙'],\n",
      " ['↖', '↙', '←', '↘', '↘', '↓', '↙', '←'],\n",
      " ['↓', '→', '→', '↗', '→', 'F', '←', '↖']]\n"
     ]
    }
   ],
   "source": [
    "r.visualize_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5, -5, -5, -4, -4, -4, -4, -3],\n",
       "       [-5, -5, -4, -4, -4, -4, -3, -3],\n",
       "       [-4, -4, -4, -4, -4, -3, -3, -3],\n",
       "       [-4, -3, -3, -3, -3, -3, -3, -3],\n",
       "       [-3, -3, -3, -3, -3, -3, -3, -3],\n",
       "       [-3, -3, -3, -2, -2, -2, -2, -2],\n",
       "       [-3, -3, -2, -2, -1, -1, -1, -2],\n",
       "       [-3, -3, -2, -2, -1,  0, -1, -1]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.agent.action_function.max(axis=2).round().astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "1. Reinforcement Learning: An Introduction  \n",
    "   Richard S. Sutton and Andrew G. Barto  \n",
    "   1st Edition  \n",
    "   MIT Press, march 1998\n",
    "2. RL Course by David Silver: Lecture playlist  \n",
    "   https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
